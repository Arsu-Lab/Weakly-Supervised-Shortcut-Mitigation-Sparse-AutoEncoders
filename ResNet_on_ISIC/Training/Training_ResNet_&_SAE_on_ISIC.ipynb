{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import *\n",
    "import PIL.Image\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from torchvision.models import AlexNet_Weights, resnet50, ResNet50_Weights, resnet18, ResNet18_Weights, resnet101, ResNet101_Weights, VGG19_Weights, vgg19\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Best settings for most CNN training\n",
    "torch.backends.cudnn.benchmark = True     # Enable auto-tuner\n",
    "torch.backends.cudnn.deterministic = False  # Allow non-deterministic ops\n",
    "torch.backends.cudnn.enabled = True        # Enable cuDNN (default)\n",
    "\n",
    "resnet_writer = SummaryWriter(log_dir='ResNet-50-full-model/resnet_isic')\n",
    "sae_writer = SummaryWriter(log_dir='ResNet-50-full-model/sae_isic')\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(23)\n",
    "np.random.seed(23)\n",
    "\n",
    "class SparseAutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, sparsity_lambda=0.70, xavier_norm_init=True):\n",
    "        super(SparseAutoEncoder, self).__init__()\n",
    "        self.sparsity_lambda = sparsity_lambda\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        if xavier_norm_init:\n",
    "            nn.init.xavier_uniform_(self.encoder[0].weight)  # Xavier initialization\n",
    "            \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            #nn.ReLU() #nn.Sigmoid()  # Output between 0-1\n",
    "        )\n",
    "        if xavier_norm_init:\n",
    "            nn.init.xavier_uniform_(self.decoder[0].weight)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "    \n",
    "    def kl_sparsity_penalty(self, encoded):\n",
    "        # Penalize the average absolute activation\n",
    "        rho_hat = torch.mean(torch.abs(encoded), dim=0)  # Average absolute activation per hidden unit\n",
    "        rho = torch.ones_like(rho_hat) * self.sparsity_target  # Target sparsity value\n",
    "        epsilon = 1e-8  # Small value to avoid log(0)\n",
    "\n",
    "        # KL-divergence computation for sparsity\n",
    "        kl_divergence = rho * torch.log(rho / (rho_hat + epsilon)) + (1 - rho) * torch.log((1 - rho) / (1 - rho_hat + epsilon))\n",
    "        kl_divergence = torch.sum(kl_divergence)  # Sum over all hidden units\n",
    "\n",
    "        return self.sparsity_lambda * kl_divergence\n",
    "\n",
    "    # L1-norm sparsity penalty calculation\n",
    "    def l1_sparsity_penalty(self, encoded):\n",
    "        # Compute the mean of absolute values of activations\n",
    "        sparsity_loss = torch.mean(torch.abs(encoded))  # Average absolute activation across all units\n",
    "        return self.sparsity_lambda * sparsity_loss  # Scale by the sparsity weight\n",
    "\n",
    "    # Loss function combining MSE (reconstruction error) and sparsity penalty\n",
    "    def loss_function(self, decoded, original, encoded):\n",
    "        mse_loss = F.mse_loss(decoded, original)  # Mean Squared Error for reconstruction\n",
    "        sparsity_loss = self.l1_sparsity_penalty(encoded)  # Sparsity penalty for hidden layer activations\n",
    "        return mse_loss + sparsity_loss  # Total loss is MSE + sparsity penalty\n",
    "\n",
    "class ISIC_Dataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \n",
    "        self.metadata = pd.read_csv(csv_file)  # Load metadata\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = f\"{self.root_dir}/{self.metadata.iloc[idx]['isic_id']}\"  # Image filename\n",
    "        image = Image.open(img_path).convert(\"RGB\")  # Load image and convert to RGB\n",
    "        # Extract label (benign=0, malignant=1)\n",
    "        benign_malignant = int(self.metadata.iloc[idx]['benign_malignant'])\n",
    "        patches = int(self.metadata.iloc[idx]['patches'])  # Assuming 1=Has patches, 0=No patches\n",
    "        label = benign_malignant # For training, we only care about benign_malignant\n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label, benign_malignant, patches # Return transformed image and label\n",
    "# 2. Transforms and Data Preparation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "# Load training and testing datasets for ISIC_dataset\n",
    "ISIC_train_dataset = ISIC_Dataset(\n",
    "    csv_file='/home/ahsan/test-project/fss/ISIC/ISIC_metadata/train_metadata.csv',\n",
    "    #csv_file='/home/ahsan/test-project/fss/ISIC/ISIC_metadata/train_metadata_sample_data.csv',\n",
    "    root_dir='/home/ahsan/test-project/fss/ISIC/ISIC_224_Dataset/isic_train',\n",
    "    transform=transform\n",
    ")\n",
    "ISIC_test_dataset = ISIC_Dataset(\n",
    "    csv_file='/home/ahsan/test-project/fss/ISIC/ISIC_metadata/test_metadata.csv',\n",
    "    #csv_file='/home/ahsan/test-project/fss/ISIC/ISIC_metadata/test_metadata_sample_data.csv',\n",
    "    root_dir='/home/ahsan/test-project/fss/ISIC/ISIC_224_Dataset/isic_test',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "ISIC_val_dataset = ISIC_Dataset(\n",
    "    #csv_file='/home/ahsan/test-project/fss/ISIC/val/metadata_ISIC_test/benign_no_yes_patch_100.csv',\n",
    "    csv_file='/home/ahsan/test-project/fss/ISIC/ISIC_metadata/val_metadata_all.csv',\n",
    "    root_dir= '/home/ahsan/test-project/fss/ISIC/ISIC_224_Dataset/isic_224/raw_224',\n",
    "    transform=transform\n",
    ")\n",
    "batch_size= 128\n",
    "ISIC_train_loader = DataLoader(ISIC_train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "ISIC_test_loader = DataLoader(ISIC_test_dataset, batch_size=1, shuffle=False, num_workers=4, pin_memory=True)\n",
    "ISIC_val_loader = DataLoader(ISIC_val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "# 3. Define the ResNet Model for Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import *\n",
    "import PIL.Image\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from torchvision.models import AlexNet_Weights, resnet50, ResNet50_Weights, resnet18, ResNet18_Weights, resnet101, ResNet101_Weights, VGG19_Weights, vgg19\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Best settings for most CNN training\n",
    "torch.backends.cudnn.benchmark = True     # Enable auto-tuner\n",
    "torch.backends.cudnn.deterministic = False  # Allow non-deterministic ops\n",
    "torch.backends.cudnn.enabled = True        # Enable cuDNN (default)\n",
    "\n",
    "resnet_writer = SummaryWriter(log_dir='ResNet-50-full-model/resnet_isic')\n",
    "sae_writer = SummaryWriter(log_dir='ResNet-50-full-model/sae_isic')\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(23)\n",
    "np.random.seed(23)\n",
    "\n",
    "class SparseAutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, sparsity_lambda=0.70, xavier_norm_init=True):\n",
    "        super(SparseAutoEncoder, self).__init__()\n",
    "        self.sparsity_lambda = sparsity_lambda\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        if xavier_norm_init:\n",
    "            nn.init.xavier_uniform_(self.encoder[0].weight)  # Xavier initialization\n",
    "            \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            #nn.ReLU() #nn.Sigmoid()  # Output between 0-1\n",
    "        )\n",
    "        if xavier_norm_init:\n",
    "            nn.init.xavier_uniform_(self.decoder[0].weight)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "    \n",
    "    def kl_sparsity_penalty(self, encoded):\n",
    "        # Penalize the average absolute activation\n",
    "        rho_hat = torch.mean(torch.abs(encoded), dim=0)  # Average absolute activation per hidden unit\n",
    "        rho = torch.ones_like(rho_hat) * self.sparsity_target  # Target sparsity value\n",
    "        epsilon = 1e-8  # Small value to avoid log(0)\n",
    "\n",
    "        # KL-divergence computation for sparsity\n",
    "        kl_divergence = rho * torch.log(rho / (rho_hat + epsilon)) + (1 - rho) * torch.log((1 - rho) / (1 - rho_hat + epsilon))\n",
    "        kl_divergence = torch.sum(kl_divergence)  # Sum over all hidden units\n",
    "\n",
    "        return self.sparsity_lambda * kl_divergence\n",
    "\n",
    "    # L1-norm sparsity penalty calculation\n",
    "    def l1_sparsity_penalty(self, encoded):\n",
    "        # Compute the mean of absolute values of activations\n",
    "        sparsity_loss = torch.mean(torch.abs(encoded))  # Average absolute activation across all units\n",
    "        return self.sparsity_lambda * sparsity_loss  # Scale by the sparsity weight\n",
    "\n",
    "    # Loss function combining MSE (reconstruction error) and sparsity penalty\n",
    "    def loss_function(self, decoded, original, encoded):\n",
    "        mse_loss = F.mse_loss(decoded, original)  # Mean Squared Error for reconstruction\n",
    "        sparsity_loss = self.l1_sparsity_penalty(encoded)  # Sparsity penalty for hidden layer activations\n",
    "        return mse_loss + sparsity_loss  # Total loss is MSE + sparsity penalty\n",
    "\n",
    "class ISIC_Dataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \n",
    "        self.metadata = pd.read_csv(csv_file)  # Load metadata\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = f\"{self.root_dir}/{self.metadata.iloc[idx]['isic_id']}\"  # Image filename\n",
    "        image = Image.open(img_path).convert(\"RGB\")  # Load image and convert to RGB\n",
    "        # Extract label (benign=0, malignant=1)\n",
    "        benign_malignant = int(self.metadata.iloc[idx]['benign_malignant'])\n",
    "        patches = int(self.metadata.iloc[idx]['patches'])  # Assuming 1=Has patches, 0=No patches\n",
    "        label = benign_malignant # For training, we only care about benign_malignant\n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label, benign_malignant, patches # Return transformed image and label\n",
    "# 2. Transforms and Data Preparation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "# Load training and testing datasets for ISIC_dataset\n",
    "ISIC_train_dataset = ISIC_Dataset(\n",
    "    csv_file='/home/ahsan/test-project/fss/ISIC/ISIC_metadata/train_metadata.csv',\n",
    "    #csv_file='/home/ahsan/test-project/fss/ISIC/ISIC_metadata/train_metadata_sample_data.csv',\n",
    "    root_dir='/home/ahsan/test-project/fss/ISIC/ISIC_224_Dataset/isic_train',\n",
    "    transform=transform\n",
    ")\n",
    "ISIC_test_dataset = ISIC_Dataset(\n",
    "    csv_file='/home/ahsan/test-project/fss/ISIC/ISIC_metadata/test_metadata.csv',\n",
    "    #csv_file='/home/ahsan/test-project/fss/ISIC/ISIC_metadata/test_metadata_sample_data.csv',\n",
    "    root_dir='/home/ahsan/test-project/fss/ISIC/ISIC_224_Dataset/isic_test',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "ISIC_val_dataset = ISIC_Dataset(\n",
    "    #csv_file='/home/ahsan/test-project/fss/ISIC/val/metadata_ISIC_test/benign_no_yes_patch_100.csv',\n",
    "    csv_file='/home/ahsan/test-project/fss/ISIC/ISIC_metadata/val_metadata_all.csv',\n",
    "    root_dir= '/home/ahsan/test-project/fss/ISIC/ISIC_224_Dataset/isic_224/raw_224',\n",
    "    transform=transform\n",
    ")\n",
    "batch_size= 128\n",
    "ISIC_train_loader = DataLoader(ISIC_train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "ISIC_test_loader = DataLoader(ISIC_test_dataset, batch_size=1, shuffle=False, num_workers=4, pin_memory=True)\n",
    "ISIC_val_loader = DataLoader(ISIC_val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "# 3. Define the ResNet Model for Binary Classification\n",
    "model = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)  # Output for 2 classes\n",
    "model = model.to(device)\n",
    "# 4. Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.05, momentum=0.9, weight_decay=1e-4, nesterov=True)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2)\n",
    "\n",
    "# 5. Training Loop\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, writer, num_epochs):\n",
    "    best_val_acc = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        for images, labels, _, _ in train_loader:\n",
    "            images, labels = images.to(device=device), labels.to(device=device)\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            # Apply gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "        \n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_acc = correct / total\n",
    "        # Validation... \n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels, _, _ in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                val_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = val_correct / val_total\n",
    "        scheduler.step(val_loss)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "        writer.add_scalar(\"Loss/val\", val_loss, epoch)\n",
    "        writer.add_scalar(\"Accuracy/train\", train_acc, epoch)\n",
    "        writer.add_scalar(\"Accuracy/val\", val_acc, epoch)\n",
    "        # Save the best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'ResNet-50-full-model-without-relu_best_model.pth')\n",
    "            print(f\"Best model saved at epoch {epoch+1} with Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "print(\"Training Model::\")\n",
    "print(\"Size of ISIC_train_loader : \", len(ISIC_train_loader)*batch_size)\n",
    "train_model(model, ISIC_train_loader, ISIC_val_loader, criterion, optimizer, resnet_writer, num_epochs=50)\n",
    "print(\"Training complete:: \")\n",
    "torch.save(model.state_dict(), 'ResNet-50-full-model-without-relu_100_epochs.pth')\n",
    "# 6. Testing for Four Classes\n",
    "def test_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    results = {\n",
    "        \"Benign_NoPatches\": 0, \"Benign_Patches\": 0, \n",
    "        \"Malignant_NoPatches\": 0, \"Malignant_Patches\": 0\n",
    "    }\n",
    "    counts = {\n",
    "        \"Benign_NoPatches\": 0, \"Benign_Patches\": 0, \n",
    "        \"Malignant_NoPatches\": 0, \"Malignant_Patches\": 0\n",
    "    }\n",
    "    with torch.no_grad():\n",
    "        for images, lables, benign_malignant, patches in test_loader:\n",
    "            images = images.to(device)\n",
    "            benign_malignant = benign_malignant.to(device).long()\n",
    "            patches = patches.to(device).long()\n",
    "\n",
    "            outputs = model(images)\n",
    "            predictions = torch.argmax(outputs, dim=1)  # Correct per-sample predictions\n",
    "\n",
    "            for label, patch, pred in zip(benign_malignant, patches, predictions):\n",
    "                if label == 0 and patch == 0:\n",
    "                    results[\"Benign_NoPatches\"] += (pred == 0).item()\n",
    "                    counts[\"Benign_NoPatches\"] += 1\n",
    "                elif label == 0 and patch == 1:\n",
    "                    results[\"Benign_Patches\"] += (pred == 0).item()\n",
    "                    counts[\"Benign_Patches\"] += 1\n",
    "                elif label == 1 and patch == 0:\n",
    "                    results[\"Malignant_NoPatches\"] += (pred == 1).item()\n",
    "                    counts[\"Malignant_NoPatches\"] += 1\n",
    "                elif label == 1 and patch == 1:\n",
    "                    results[\"Malignant_Patches\"] += (pred == 1).item()\n",
    "                    counts[\"Malignant_Patches\"] += 1\n",
    "            \n",
    "    # Print subgroup-wise accuracy\n",
    "    for key in results:\n",
    "        if counts[key] > 0:\n",
    "            accuracy = results[key] / counts[key]\n",
    "            print(f\"Accuracy for {key}: {accuracy:.4f}\")\n",
    "        else:\n",
    "            print(f\"No samples for {key}\")\n",
    "# Example usage\n",
    "print(\"Testing Model::\")\n",
    "print(\"size of ISIC_test_loader : \", len(ISIC_test_loader)*batch_size)\n",
    "test_model(model, ISIC_test_loader, device=device)\n",
    "\n",
    "# Optional: Save activations (for last layer before fc)\n",
    "def save_activations(model, dataloader, save_path):\n",
    "    model.eval()\n",
    "    activations = []\n",
    "    def hook_fn(module, input, output):\n",
    "        activations.append(output.detach().cpu())\n",
    "    handle = model.avgpool.register_forward_hook(hook_fn)\n",
    "    with torch.no_grad():\n",
    "        for images, _, _, _ in dataloader:\n",
    "            model(images.to(device))\n",
    "    handle.remove()\n",
    "    act_tensor = torch.cat(activations, dim=0).squeeze()  # shape: (N, 2048)\n",
    "    np.save(save_path + \".npy\", act_tensor.numpy())\n",
    "    pd.DataFrame(act_tensor.numpy()).to_csv(save_path + \".csv\", index=False)\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load(\"/home/ahsan/test-project/fss/split-metadata/testing-resNet-model/ResNet-results-without-relu/ResNet-50-full-model/ResNet-on_ISIC/ResNet-ISIC-full-training-testing/ResNet-50-full-model-without-relu_best_model.pth\", map_location=device))\n",
    "model.eval()\n",
    "# Save activations for test set\n",
    "save_activations(model, ISIC_train_loader, \"ResNet-50-full-model-without-relu-activations_100_epochs\")\n",
    "print(\"isic_train_activations save successfully!\")\n",
    "# SAE Train Loop\n",
    "def train_sae(model, data, epochs, lr, batch_size, writer):\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-3)\n",
    "\n",
    "    dataloader = DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch in dataloader:\n",
    "            batch = torch.stack(batch).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            encoded, decoded = model(batch)\n",
    "            loss = model.loss_function(decoded, batch, encoded)\n",
    "            #loss = criterion(decoded, batch) + sparsity_loss(encoded, sparsity_lambda)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        sae_writer.add_scalar(\"Loss/train\", avg_loss, epoch)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], SAE Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "\n",
    "# Load previously saved activations\n",
    "activations_np = np.load(\"ResNet-50-full-model-without-relu-activations_100_epochs.npy\")\n",
    "activations = torch.tensor(activations_np, dtype=torch.float32)\n",
    "# Wrap into a TensorDataset\n",
    "activation_dataset = TensorDataset(activations)\n",
    "\n",
    "sae = SparseAutoEncoder(input_dim=2048, hidden_dim=8000)\n",
    "train_sae(sae, activation_dataset, epochs=100, lr=0.001, batch_size=128, writer=sae_writer)\n",
    "torch.save(sae.state_dict(), 'ResNet-50-full-model-without-relu-SAE_100_epochs.pth')\n",
    "print(\"Training SAE complete!!\")\n",
    "resnet_writer.close()\n",
    "sae_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3794546/1659364844.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('ResNet_ISIC_seed-1.pth', map_location=device))  # or 'cuda' if using GPU\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet50(weights=None)  # Don't load pretrained weights  # Binary classification\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)\n",
    "model.load_state_dict(torch.load('ResNet_ISIC_seed-1.pth', map_location=device))  # or 'cuda' if using GPU\n",
    "model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3794546/3921347835.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"ResNet_ISIC_seed-1.pth\", map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isic_train_activations save successfully!\n",
      "Epoch [1/100], SAE Loss: 0.0472\n",
      "Epoch [2/100], SAE Loss: 0.0408\n",
      "Epoch [3/100], SAE Loss: 0.0401\n",
      "Epoch [4/100], SAE Loss: 0.0397\n",
      "Epoch [5/100], SAE Loss: 0.0395\n",
      "Epoch [6/100], SAE Loss: 0.0394\n",
      "Epoch [7/100], SAE Loss: 0.0393\n",
      "Epoch [8/100], SAE Loss: 0.0393\n",
      "Epoch [9/100], SAE Loss: 0.0393\n",
      "Epoch [10/100], SAE Loss: 0.0392\n",
      "Epoch [11/100], SAE Loss: 0.0393\n",
      "Epoch [12/100], SAE Loss: 0.0392\n",
      "Epoch [13/100], SAE Loss: 0.0392\n",
      "Epoch [14/100], SAE Loss: 0.0392\n",
      "Epoch [15/100], SAE Loss: 0.0392\n",
      "Epoch [16/100], SAE Loss: 0.0393\n",
      "Epoch [17/100], SAE Loss: 0.0392\n",
      "Epoch [18/100], SAE Loss: 0.0392\n",
      "Epoch [19/100], SAE Loss: 0.0392\n",
      "Epoch [20/100], SAE Loss: 0.0392\n",
      "Epoch [21/100], SAE Loss: 0.0392\n",
      "Epoch [22/100], SAE Loss: 0.0392\n",
      "Epoch [23/100], SAE Loss: 0.0392\n",
      "Epoch [24/100], SAE Loss: 0.0392\n",
      "Epoch [25/100], SAE Loss: 0.0392\n",
      "Epoch [26/100], SAE Loss: 0.0392\n",
      "Epoch [27/100], SAE Loss: 0.0392\n",
      "Epoch [28/100], SAE Loss: 0.0392\n",
      "Epoch [29/100], SAE Loss: 0.0392\n",
      "Epoch [30/100], SAE Loss: 0.0392\n",
      "Epoch [31/100], SAE Loss: 0.0392\n",
      "Epoch [32/100], SAE Loss: 0.0392\n",
      "Epoch [33/100], SAE Loss: 0.0392\n",
      "Epoch [34/100], SAE Loss: 0.0392\n",
      "Epoch [35/100], SAE Loss: 0.0392\n",
      "Epoch [36/100], SAE Loss: 0.0392\n",
      "Epoch [37/100], SAE Loss: 0.0392\n",
      "Epoch [38/100], SAE Loss: 0.0392\n",
      "Epoch [39/100], SAE Loss: 0.0392\n",
      "Epoch [40/100], SAE Loss: 0.0392\n",
      "Epoch [41/100], SAE Loss: 0.0392\n",
      "Epoch [42/100], SAE Loss: 0.0392\n",
      "Epoch [43/100], SAE Loss: 0.0392\n",
      "Epoch [44/100], SAE Loss: 0.0392\n",
      "Epoch [45/100], SAE Loss: 0.0392\n",
      "Epoch [46/100], SAE Loss: 0.0392\n",
      "Epoch [47/100], SAE Loss: 0.0392\n",
      "Epoch [48/100], SAE Loss: 0.0392\n",
      "Epoch [49/100], SAE Loss: 0.0392\n",
      "Epoch [50/100], SAE Loss: 0.0392\n",
      "Epoch [51/100], SAE Loss: 0.0392\n",
      "Epoch [52/100], SAE Loss: 0.0392\n",
      "Epoch [53/100], SAE Loss: 0.0392\n",
      "Epoch [54/100], SAE Loss: 0.0392\n",
      "Epoch [55/100], SAE Loss: 0.0392\n",
      "Epoch [56/100], SAE Loss: 0.0392\n",
      "Epoch [57/100], SAE Loss: 0.0392\n",
      "Epoch [58/100], SAE Loss: 0.0392\n",
      "Epoch [59/100], SAE Loss: 0.0392\n",
      "Epoch [60/100], SAE Loss: 0.0392\n",
      "Epoch [61/100], SAE Loss: 0.0392\n",
      "Epoch [62/100], SAE Loss: 0.0392\n",
      "Epoch [63/100], SAE Loss: 0.0392\n",
      "Epoch [64/100], SAE Loss: 0.0392\n",
      "Epoch [65/100], SAE Loss: 0.0392\n",
      "Epoch [66/100], SAE Loss: 0.0392\n",
      "Epoch [67/100], SAE Loss: 0.0392\n",
      "Epoch [68/100], SAE Loss: 0.0392\n",
      "Epoch [69/100], SAE Loss: 0.0392\n",
      "Epoch [70/100], SAE Loss: 0.0392\n",
      "Epoch [71/100], SAE Loss: 0.0392\n",
      "Epoch [72/100], SAE Loss: 0.0392\n",
      "Epoch [73/100], SAE Loss: 0.0392\n",
      "Epoch [74/100], SAE Loss: 0.0392\n",
      "Epoch [75/100], SAE Loss: 0.0392\n",
      "Epoch [76/100], SAE Loss: 0.0392\n",
      "Epoch [77/100], SAE Loss: 0.0392\n",
      "Epoch [78/100], SAE Loss: 0.0392\n",
      "Epoch [79/100], SAE Loss: 0.0392\n",
      "Epoch [80/100], SAE Loss: 0.0392\n",
      "Epoch [81/100], SAE Loss: 0.0392\n",
      "Epoch [82/100], SAE Loss: 0.0392\n",
      "Epoch [83/100], SAE Loss: 0.0392\n",
      "Epoch [84/100], SAE Loss: 0.0392\n",
      "Epoch [85/100], SAE Loss: 0.0392\n",
      "Epoch [86/100], SAE Loss: 0.0392\n",
      "Epoch [87/100], SAE Loss: 0.0392\n",
      "Epoch [88/100], SAE Loss: 0.0392\n",
      "Epoch [89/100], SAE Loss: 0.0392\n",
      "Epoch [90/100], SAE Loss: 0.0392\n",
      "Epoch [91/100], SAE Loss: 0.0392\n",
      "Epoch [92/100], SAE Loss: 0.0392\n",
      "Epoch [93/100], SAE Loss: 0.0392\n",
      "Epoch [94/100], SAE Loss: 0.0392\n",
      "Epoch [95/100], SAE Loss: 0.0392\n",
      "Epoch [96/100], SAE Loss: 0.0392\n",
      "Epoch [97/100], SAE Loss: 0.0392\n",
      "Epoch [98/100], SAE Loss: 0.0392\n",
      "Epoch [99/100], SAE Loss: 0.0392\n",
      "Epoch [100/100], SAE Loss: 0.0392\n",
      "Training SAE complete!!\n"
     ]
    }
   ],
   "source": [
    "def save_activations(model, dataloader, save_path):\n",
    "    model.eval()\n",
    "    activations = []\n",
    "    def hook_fn(module, input, output):\n",
    "        activations.append(output.detach().cpu())\n",
    "    handle = model.avgpool.register_forward_hook(hook_fn)\n",
    "    with torch.no_grad():\n",
    "        for images, _, _, _ in dataloader:\n",
    "            model(images.to(device))\n",
    "    handle.remove()\n",
    "    act_tensor = torch.cat(activations, dim=0).squeeze()  # shape: (N, 2048)\n",
    "    np.save(save_path + \".npy\", act_tensor.numpy())\n",
    "    pd.DataFrame(act_tensor.numpy()).to_csv(save_path + \".csv\", index=False)\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load(\"ResNet_ISIC_seed-1.pth\", map_location=device))\n",
    "model.eval()\n",
    "# Save activations for test set\n",
    "save_activations(model, ISIC_train_loader, \"ResNet-50-full-model-without-relu-activations_100_epochs\")\n",
    "print(\"isic_train_activations save successfully!\")\n",
    "# SAE Train Loop\n",
    "def train_sae(model, data, epochs, lr, batch_size, writer):\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-3)\n",
    "\n",
    "    dataloader = DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch in dataloader:\n",
    "            batch = torch.stack(batch).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            encoded, decoded = model(batch)\n",
    "            loss = model.loss_function(decoded, batch, encoded)\n",
    "            #loss = criterion(decoded, batch) + sparsity_loss(encoded, sparsity_lambda)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        sae_writer.add_scalar(\"Loss/train\", avg_loss, epoch)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], SAE Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "\n",
    "# Load previously saved activations\n",
    "activations_np = np.load(\"ResNet-50-full-model-without-relu-activations_100_epochs.npy\")\n",
    "activations = torch.tensor(activations_np, dtype=torch.float32)\n",
    "# Wrap into a TensorDataset\n",
    "activation_dataset = TensorDataset(activations)\n",
    "\n",
    "sae = SparseAutoEncoder(input_dim=2048, hidden_dim=8000)\n",
    "train_sae(sae, activation_dataset, epochs=100, lr=0.001, batch_size=128, writer=sae_writer)\n",
    "torch.save(sae.state_dict(), 'ResNet-50-full-model-without-relu-SAE_100_epochs.pth')\n",
    "print(\"Training SAE complete!!\")\n",
    "resnet_writer.close()\n",
    "sae_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on ISIC test dataset...\n",
      "Accuracy for Benign_NoPatches: 0.8404\n",
      "Accuracy for Benign_Patches: 0.9946\n",
      "Accuracy for Malignant_NoPatches: 0.5389\n",
      "Accuracy for Malignant_Patches: 0.2765\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#test\n",
    "def test_model(model, dataloader):\n",
    "    results = {\n",
    "        \"Benign_NoPatches\": 0, \"Benign_Patches\": 0,\n",
    "        \"Malignant_NoPatches\": 0, \"Malignant_Patches\": 0\n",
    "    }\n",
    "    counts = {\n",
    "        \"Benign_NoPatches\": 0, \"Benign_Patches\": 0,\n",
    "        \"Malignant_NoPatches\": 0, \"Malignant_Patches\": 0\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels, benign_malignant, patches in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            for i in range(labels.size(0)):\n",
    "                label = int(benign_malignant[i])\n",
    "                patch = int(patches[i])\n",
    "                pred = int(predictions[i])\n",
    "\n",
    "                key = f\"{'Benign' if label == 0 else 'Malignant'}_{'NoPatches' if patch == 0 else 'Patches'}\"\n",
    "                correct_class = (pred == label)\n",
    "                results[key] += int(correct_class)\n",
    "                counts[key] += 1\n",
    "\n",
    "    # Print subgroup-wise accuracy\n",
    "    for key in results:\n",
    "        if counts[key] > 0:\n",
    "            accuracy = results[key] / counts[key]\n",
    "            print(f\"Accuracy for {key}: {accuracy:.4f}\")\n",
    "        else:\n",
    "            print(f\"No samples for {key}\")\n",
    "            \n",
    "print(\"Evaluating model on ISIC test dataset...\")\n",
    "test_model(model, ISIC_test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Model::\n",
      "size of ISIC_test_loader :  968192\n",
      "Accuracy for Benign_NoPatches: 0.7128\n",
      "Accuracy for Benign_Patches: 0.9989\n",
      "Accuracy for Malignant_NoPatches: 0.7494\n",
      "Accuracy for Malignant_Patches: 0.2850\n"
     ]
    }
   ],
   "source": [
    "def test_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    results = {\n",
    "        \"Benign_NoPatches\": 0, \"Benign_Patches\": 0, \n",
    "        \"Malignant_NoPatches\": 0, \"Malignant_Patches\": 0\n",
    "    }\n",
    "    counts = {\n",
    "        \"Benign_NoPatches\": 0, \"Benign_Patches\": 0, \n",
    "        \"Malignant_NoPatches\": 0, \"Malignant_Patches\": 0\n",
    "    }\n",
    "    with torch.no_grad():\n",
    "        for images, lables, benign_malignant, patches in test_loader:\n",
    "            images = images.to(device)\n",
    "            benign_malignant = benign_malignant.to(device).long()\n",
    "            patches = patches.to(device).long()\n",
    "\n",
    "            outputs = model(images)\n",
    "            predictions = torch.argmax(outputs, dim=1)  # Correct per-sample predictions\n",
    "\n",
    "            for label, patch, pred in zip(benign_malignant, patches, predictions):\n",
    "                if label == 0 and patch == 0:\n",
    "                    results[\"Benign_NoPatches\"] += (pred == 0).item()\n",
    "                    counts[\"Benign_NoPatches\"] += 1\n",
    "                elif label == 0 and patch == 1:\n",
    "                    results[\"Benign_Patches\"] += (pred == 0).item()\n",
    "                    counts[\"Benign_Patches\"] += 1\n",
    "                elif label == 1 and patch == 0:\n",
    "                    results[\"Malignant_NoPatches\"] += (pred == 1).item()\n",
    "                    counts[\"Malignant_NoPatches\"] += 1\n",
    "                elif label == 1 and patch == 1:\n",
    "                    results[\"Malignant_Patches\"] += (pred == 1).item()\n",
    "                    counts[\"Malignant_Patches\"] += 1\n",
    "            \n",
    "    # Print subgroup-wise accuracy\n",
    "    for key in results:\n",
    "        if counts[key] > 0:\n",
    "            accuracy = results[key] / counts[key]\n",
    "            print(f\"Accuracy for {key}: {accuracy:.4f}\")\n",
    "        else:\n",
    "            print(f\"No samples for {key}\")\n",
    "# Example usage\n",
    "print(\"Testing Model::\")\n",
    "print(\"size of ISIC_test_loader : \", len(ISIC_test_loader)*batch_size)\n",
    "test_model(model, ISIC_test_loader, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xAI-bio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
