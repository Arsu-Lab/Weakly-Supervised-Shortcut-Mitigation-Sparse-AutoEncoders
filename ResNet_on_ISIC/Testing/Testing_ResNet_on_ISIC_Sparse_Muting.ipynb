{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from torchvision.models import alexnet, AlexNet_Weights, resnet50, ResNet50_Weights, resnet18, ResNet18_Weights, resnet101, ResNet101_Weights, VGG19_Weights, vgg19\n",
    "from PIL import *\n",
    "import PIL.Image\n",
    "import gc\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(23)\n",
    "np.random.seed(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Datasets loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# 1. Define Datasets and Dataloader\n",
    "class ISIC_Dataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (str): Path to the metadata CSV file.\n",
    "            root_dir (str): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on an image.\n",
    "        \"\"\"\n",
    "        self.metadata = pd.read_csv(csv_file)  # Load metadata\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = f\"{self.root_dir}/{self.metadata.iloc[idx]['isic_id']}\"  # Image filename\n",
    "        image = Image.open(img_path).convert(\"RGB\")  # Load image and convert to RGB\n",
    "\n",
    "        # Extract label (benign=0, malignant=1)\n",
    "        benign_malignant = self.metadata.iloc[idx]['benign_malignant'] \n",
    "        patches = self.metadata.iloc[idx]['patches']  # Assuming 1=Has patches, 0=No patches\n",
    "        label = benign_malignant # For training, we only care about benign_malignant\n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label, benign_malignant, patches # Return transformed image and label\n",
    "\n",
    "\n",
    "# 2. Transforms and Data Preparation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "# Load Test dataset   /home/ahsan/test-project/fss/ISIC/val/metadata_ISIC_test/benign_no_yes_patch_100.csv\n",
    "ISIC_test_dataset = ISIC_Dataset(\n",
    "    csv_file='/home/ahsan/test-project/fss/ISIC/ISIC_metadata/test_metadata.csv',\n",
    "    root_dir='/home/ahsan/test-project/fss/ISIC/ISIC_224_Dataset/isic_test',\n",
    "    #root_dir='/home/ahsan/test-project/fss/ISIC/ISIC_224_Dataset/isic_224/raw_224',\n",
    "    transform=transform\n",
    ")\n",
    "ISIC_test_dataset_benign = ISIC_Dataset(\n",
    "    csv_file='/home/ahsan/test-project/fss/ISIC/ISIC_metadata/test_metadata_benign.csv',\n",
    "    root_dir='/home/ahsan/test-project/fss/ISIC/ISIC_224_Dataset/isic_test',\n",
    "    transform=transform\n",
    ")\n",
    "ISIC_test_dataset_malignant = ISIC_Dataset(\n",
    "    csv_file='/home/ahsan/test-project/fss/ISIC/ISIC_metadata/test_metadata_malignant.csv',\n",
    "    root_dir='/home/ahsan/test-project/fss/ISIC/ISIC_224_Dataset/isic_test',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "ISIC_test_dataset_malignant_patch = ISIC_Dataset(\n",
    "    csv_file='/home/ahsan/test-project/fss/ISIC/ISIC_metadata/test_metadata_malignant_patch.csv',\n",
    "    root_dir='/home/ahsan/test-project/fss/ISIC/ISIC_224_Dataset/isic_test',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "ISIC_test_dataset_malignant_no_patch = ISIC_Dataset(\n",
    "    csv_file='/home/ahsan/test-project/fss/ISIC/ISIC_metadata/test_metadata_malignant_no_patch.csv',\n",
    "    root_dir='/home/ahsan/test-project/fss/ISIC/ISIC_224_Dataset/isic_test',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "ISIC_test_dataset_benign_patch = ISIC_Dataset(\n",
    "    csv_file='/home/ahsan/test-project/fss/ISIC/ISIC_metadata/test_metadata_benign_patch.csv',\n",
    "    root_dir='/home/ahsan/test-project/fss/ISIC/ISIC_224_Dataset/isic_test',\n",
    "    transform=transform\n",
    ")\n",
    "ISIC_test_dataset_benign_no_patch = ISIC_Dataset(\n",
    "    csv_file='/home/ahsan/test-project/fss/ISIC/ISIC_metadata/test_metadata_benign_no_patch.csv',\n",
    "    root_dir='/home/ahsan/test-project/fss/ISIC/ISIC_224_Dataset/isic_test',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "ISIC_test_benign_no_yes_100 = ISIC_Dataset(\n",
    "    csv_file='/home/ahsan/test-project/fss/ISIC/val/metadata_ISIC_test/benign_no_yes_patch_100.csv',\n",
    "    root_dir='/home/ahsan/test-project/fss/ISIC/test-100/benign_no_yes_patch_100',\n",
    "    transform=transform\n",
    ")\n",
    "ISIC_test_malignant_no_yes_100 = ISIC_Dataset(\n",
    "    csv_file='/home/ahsan/test-project/fss/ISIC/val/metadata_ISIC_test/malignant_no_yes_patch_100.csv',\n",
    "    root_dir='/home/ahsan/test-project/fss/ISIC/test-100/malignant_no_yes_patch_100',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Load validation dataset\n",
    "ISIC_val_dataset = ISIC_Dataset(\n",
    "    #csv_file='/home/ahsan/test-project/fss/split-metadata/output_metadata/val_metadata_updated_LB.csv',\n",
    "    csv_file='/home/ahsan/test-project/fss/ISIC/ISIC_metadata/val_metadata_all.csv',\n",
    "    root_dir='/home/ahsan/test-project/fss/ISIC/ISIC_224_Dataset/isic_224/raw_224',\n",
    "    transform=transform\n",
    ")\n",
    "ISIC_val_benign = ISIC_Dataset(\n",
    "    #csv_file='/home/ahsan/test-project/fss/split-metadata/output_metadata/val_metadata_updated_LB.csv',\n",
    "    csv_file='/home/ahsan/test-project/fss/ISIC/ISIC_metadata/val_metadata_benign.csv',\n",
    "    root_dir='/home/ahsan/test-project/fss/ISIC/ISIC_224_Dataset/isic_224/raw_224',\n",
    "    transform=transform\n",
    ")\n",
    "ISIC_val_malignant = ISIC_Dataset(\n",
    "    #csv_file='/home/ahsan/test-project/fss/split-metadata/output_metadata/val_metadata_updated_LB.csv',\n",
    "    csv_file='/home/ahsan/test-project/fss/ISIC/ISIC_metadata/val_metadata_malignant.csv',\n",
    "    root_dir='/home/ahsan/test-project/fss/ISIC/ISIC_224_Dataset/isic_224/raw_224',\n",
    "    transform=transform\n",
    ")\n",
    "# Diffrent variations of Validation-ISIC\n",
    "ISIC_val_benign_no_yes_patch_25 = ISIC_Dataset(\n",
    "    csv_file='/home/ahsan/test-project/fss/ISIC/val/metadata_ISIC_val/isic_benign_no_yes_patch_25.csv',\n",
    "    root_dir='/home/ahsan/test-project/fss/ISIC/val/benign_no_patch_25',\n",
    "    transform=transform\n",
    ")\n",
    "ISIC_val_benign_patch_25 = ISIC_Dataset(\n",
    "    csv_file='/home/ahsan/test-project/fss/ISIC/val/metadata_ISIC_val/isic_benign_patch_25.csv',\n",
    "    #csv_file='/home/ahsan/test-project/fss/ISIC/val/metadata_ISIC_val/isic_benign_patch_5.csv',\n",
    "    root_dir='/home/ahsan/test-project/fss/ISIC/val/benign_no_patch_25',\n",
    "    transform=transform\n",
    ")\n",
    "ISIC_val_benign_no_patch_25 = ISIC_Dataset(\n",
    "    csv_file='/home/ahsan/test-project/fss/ISIC/val/metadata_ISIC_val/isic_benign_no_patch_25.csv',\n",
    "    #csv_file='/home/ahsan/test-project/fss/ISIC/val/metadata_ISIC_val/isic_benign_no_patch_5.csv',\n",
    "    root_dir='/home/ahsan/test-project/fss/ISIC/val/benign_no_patch_25',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "\n",
    "ISIC_val_malignant_no_yes_patch_25 = ISIC_Dataset(\n",
    "    csv_file='/home/ahsan/test-project/fss/ISIC/val/metadata_ISIC_val/isic_malignant_no_yes_patch_25.csv',\n",
    "    root_dir='/home/ahsan/test-project/fss/ISIC/val/malignant_patch_25',\n",
    "    transform=transform\n",
    ")\n",
    "ISIC_val_malignant_no_patch_25 = ISIC_Dataset(\n",
    "    csv_file='/home/ahsan/test-project/fss/ISIC/val/metadata_ISIC_val/isic_malignant_no_patch_25.csv',\n",
    "    #csv_file='/home/ahsan/test-project/fss/ISIC/val/metadata_ISIC_val/isic_malignant_no_patch_5.csv',\n",
    "    root_dir='/home/ahsan/test-project/fss/ISIC/val/malignant_patch_25',\n",
    "    transform=transform\n",
    ")\n",
    "ISIC_val_malignant_patch_25 = ISIC_Dataset(\n",
    "    csv_file='/home/ahsan/test-project/fss/ISIC/val/metadata_ISIC_val/isic_malignant_patch_25.csv',\n",
    "    #csv_file='/home/ahsan/test-project/fss/ISIC/val/metadata_ISIC_val/isic_malignant_patch_5.csv',\n",
    "    root_dir='/home/ahsan/test-project/fss/ISIC/val/malignant_patch_25',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "\n",
    "ISIC_val_benign_no_yes_patch_50 = ISIC_Dataset(\n",
    "    csv_file='/home/ahsan/test-project/fss/ISIC/val/metadata_ISIC_val/isic_benign_no_yes_patch_50.csv',\n",
    "    root_dir='/home/ahsan/test-project/fss/ISIC/val/benign_no_patch_50',\n",
    "    transform=transform\n",
    ")\n",
    "ISIC_val_benign_patch_50 = ISIC_Dataset(\n",
    "    csv_file='/home/ahsan/test-project/fss/ISIC/val/metadata_ISIC_val/isic_benign_patch_50.csv',\n",
    "    root_dir='/home/ahsan/test-project/fss/ISIC/val/benign_no_patch_50',\n",
    "    transform=transform\n",
    ")\n",
    "ISIC_val_benign_no_patch_50 = ISIC_Dataset(\n",
    "    csv_file='/home/ahsan/test-project/fss/ISIC/val/metadata_ISIC_val/isic_benign_no_patch_50.csv',\n",
    "    root_dir='/home/ahsan/test-project/fss/ISIC/val/benign_no_patch_50',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "\n",
    "ISIC_val_malignant_no_yes_patch_50 = ISIC_Dataset(\n",
    "    csv_file='/home/ahsan/test-project/fss/ISIC/val/metadata_ISIC_val/isic_malignant_no_yes_patch_50.csv',\n",
    "    root_dir='/home/ahsan/test-project/fss/ISIC/val/malignant_no_patch_50',\n",
    "    transform=transform\n",
    ")\n",
    "ISIC_val_malignant_patch_50 = ISIC_Dataset(\n",
    "    csv_file='/home/ahsan/test-project/fss/ISIC/val/metadata_ISIC_val/isic_malignant_patch_50.csv',\n",
    "    root_dir='/home/ahsan/test-project/fss/ISIC/val/malignant_no_patch_50',\n",
    "    transform=transform\n",
    ")\n",
    "ISIC_val_malignant_no_patch_50 = ISIC_Dataset(\n",
    "    csv_file='/home/ahsan/test-project/fss/ISIC/val/metadata_ISIC_val/isic_malignant_no_patch_50.csv',\n",
    "    root_dir='/home/ahsan/test-project/fss/ISIC/val/malignant_no_patch_50',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "\n",
    "ISIC_val_benign_no_yes_patch_100 = ISIC_Dataset(\n",
    "    csv_file='/home/ahsan/test-project/fss/ISIC/val/metadata_ISIC_val/isic_benign_no_yes_patch_100.csv',\n",
    "    root_dir='/home/ahsan/test-project/fss/ISIC/val/benign_no_patch_100',\n",
    "    transform=transform\n",
    ")\n",
    "ISIC_val_benign_no_patch_100 = ISIC_Dataset(\n",
    "    csv_file='/home/ahsan/test-project/fss/ISIC/val/metadata_ISIC_val/isic_benign_no_patch_100.csv',\n",
    "    root_dir='/home/ahsan/test-project/fss/ISIC/val/benign_no_patch_100',\n",
    "    transform=transform\n",
    ")\n",
    "ISIC_val_benign_patch_100 = ISIC_Dataset(\n",
    "    csv_file='/home/ahsan/test-project/fss/ISIC/val/metadata_ISIC_val/isic_benign_patch_100.csv',\n",
    "    root_dir='/home/ahsan/test-project/fss/ISIC/val/benign_no_patch_100',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "ISIC_val_malignant_no_yes_patch_100 = ISIC_Dataset(\n",
    "    csv_file='/home/ahsan/test-project/fss/ISIC/val/metadata_ISIC_val/isic_malignant_no_yes_patch_100.csv',\n",
    "    root_dir='/home/ahsan/test-project/fss/ISIC/val/malignant_no_patch_100',\n",
    "    transform=transform\n",
    ")\n",
    "ISIC_val_malignant_patch_100 = ISIC_Dataset(\n",
    "    csv_file='/home/ahsan/test-project/fss/ISIC/val/metadata_ISIC_val/isic_malignant_patch_100.csv',\n",
    "    root_dir='/home/ahsan/test-project/fss/ISIC/val/malignant_no_patch_100',\n",
    "    transform=transform\n",
    ")\n",
    "ISIC_val_malignant_no_patch_100 = ISIC_Dataset(\n",
    "    csv_file='/home/ahsan/test-project/fss/ISIC/val/metadata_ISIC_val/isic_malignant_no_patch_100.csv',\n",
    "    root_dir='/home/ahsan/test-project/fss/ISIC/val/malignant_no_patch_100',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "ISIC_val_benign_no_yes_patch_200 = ISIC_Dataset(\n",
    "    csv_file='/home/ahsan/test-project/fss/ISIC/val/metadata_ISIC_val/isic_benign_no_yes_patch_200.csv',\n",
    "    root_dir='/home/ahsan/test-project/fss/ISIC/val/benign_no_patch_200',\n",
    "    transform=transform\n",
    ")\n",
    "ISIC_val_benign_no_patch_200 = ISIC_Dataset(\n",
    "    csv_file='/home/ahsan/test-project/fss/ISIC/val/metadata_ISIC_val/isic_benign_no_patch_200.csv',\n",
    "    root_dir='/home/ahsan/test-project/fss/ISIC/val/benign_no_patch_200',\n",
    "    transform=transform\n",
    ")\n",
    "ISIC_val_benign_patch_200 = ISIC_Dataset(\n",
    "    csv_file='/home/ahsan/test-project/fss/ISIC/val/metadata_ISIC_val/isic_benign_patch_200.csv',\n",
    "    root_dir='/home/ahsan/test-project/fss/ISIC/val/benign_no_patch_200',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "ISIC_val_malignant_no_yes_patch_200 = ISIC_Dataset(\n",
    "    csv_file='/home/ahsan/test-project/fss/ISIC/val/metadata_ISIC_val/isic_malignant_no_yes_patch_200.csv',\n",
    "    root_dir='/home/ahsan/test-project/fss/ISIC/val/malignant_no_patch_200',\n",
    "    transform=transform\n",
    ")\n",
    "ISIC_val_malignant_patch_200 = ISIC_Dataset(\n",
    "    csv_file='/home/ahsan/test-project/fss/ISIC/val/metadata_ISIC_val/isic_malignant_patch_200.csv',\n",
    "    root_dir='/home/ahsan/test-project/fss/ISIC/val/malignant_no_patch_200',\n",
    "    transform=transform\n",
    ")\n",
    "ISIC_val_malignant_no_patch_200 = ISIC_Dataset(\n",
    "    csv_file='/home/ahsan/test-project/fss/ISIC/val/metadata_ISIC_val/isic_malignant_no_patch_200.csv',\n",
    "    root_dir='/home/ahsan/test-project/fss/ISIC/val/malignant_no_patch_200',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "ISIC_testLoader_dataset = DataLoader(ISIC_test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "ISIC_testLoader_dataset_benign = DataLoader(ISIC_test_dataset_benign, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "ISIC_testLoader_dataset_malignant = DataLoader(ISIC_test_dataset_malignant, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "ISIC_testLoader_dataset_benign_patch = DataLoader(ISIC_test_dataset_benign_patch, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "ISIC_testLoader_dataset_malignant_patch = DataLoader(ISIC_test_dataset_malignant_patch, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "ISIC_testLoader_dataset_benign_no_patch = DataLoader(ISIC_test_dataset_benign_no_patch, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "ISIC_testLoader_dataset_malignant_no_patch = DataLoader(ISIC_test_dataset_malignant_no_patch, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "ISIC_testLoader_benign_no_yes_100 = DataLoader(ISIC_test_benign_no_yes_100, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "ISIC_testLoader_malignant_no_yes_100 = DataLoader(ISIC_test_malignant_no_yes_100, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "ISIC_valLoader_benign_no_yes_patch_25 = DataLoader(ISIC_val_benign_no_yes_patch_25, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "ISIC_valLoader_malignant_no_yes_patch_25 = DataLoader(ISIC_val_malignant_no_yes_patch_25, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "ISIC_valLoader_benign_no_yes_patch_50 = DataLoader(ISIC_val_benign_no_yes_patch_50, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "ISIC_valLoader_malignant_no_yes_patch_50 = DataLoader(ISIC_val_malignant_no_yes_patch_50, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "ISIC_valLoader_benign_no_yes_patch_100 = DataLoader(ISIC_val_benign_no_yes_patch_100, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "ISIC_valLoader_malignant_no_yes_patch_100 = DataLoader(ISIC_val_malignant_no_yes_patch_100, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "ISIC_valLoader_benign_no_yes_patch_200 = DataLoader(ISIC_val_benign_no_yes_patch_200, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "ISIC_valLoader_malignant_no_yes_patch_200 = DataLoader(ISIC_val_malignant_no_yes_patch_200, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "ISIC_valLoader_malignant_no_patch_200 = DataLoader(ISIC_val_malignant_no_patch_200, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "ISIC_valLoader_malignant_patch_200 = DataLoader(ISIC_val_malignant_patch_200, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "ISIC_valLoader_benign_no_patch_200 = DataLoader(ISIC_val_benign_no_patch_200, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "ISIC_valLoader_benign_patch_200 = DataLoader(ISIC_val_benign_patch_200, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "ISIC_valLoader_benign_no_patch_100 = DataLoader(ISIC_val_benign_no_patch_100, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "ISIC_valLoader_benign_patch_100 = DataLoader(ISIC_val_benign_patch_100, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "ISIC_valLoader_malignant_no_patch_100 = DataLoader(ISIC_val_malignant_no_patch_100, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "ISIC_valLoader_malignant_patch_100 = DataLoader(ISIC_val_malignant_patch_100, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "ISIC_valLoader_benign_no_patch_50 = DataLoader(ISIC_val_benign_no_patch_50, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "ISIC_valLoader_benign_patch_50 = DataLoader(ISIC_val_benign_patch_50, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "ISIC_valLoader_malignant_no_patch_50 = DataLoader(ISIC_val_malignant_no_patch_50, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "ISIC_valLoader_malignant_patch_50 = DataLoader(ISIC_val_malignant_patch_50, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "ISIC_valLoader_benign_no_patch_25 = DataLoader(ISIC_val_benign_no_patch_25, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "ISIC_valLoader_benign_patch_25 = DataLoader(ISIC_val_benign_patch_25, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "ISIC_valLoader_malignant_patch_25 = DataLoader(ISIC_val_malignant_patch_25, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "ISIC_valLoader_malignant_no_patch_25 = DataLoader(ISIC_val_malignant_no_patch_25, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "\n",
    "ISIC_val_loader = DataLoader(ISIC_val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "ISIC_val_benign = DataLoader(ISIC_val_benign, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "ISIC_val_malignant = DataLoader(ISIC_val_malignant, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "print(\"All Datasets loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseAutoEncoder_2(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, sparsity_lambda=0.7, xavier_norm_init=True):\n",
    "        super(SparseAutoEncoder_2, self).__init__()\n",
    "        self.sparsity_lambda = sparsity_lambda\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        if xavier_norm_init:\n",
    "            nn.init.xavier_uniform_(self.encoder[0].weight)  # Xavier initialization\n",
    "            \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            #nn.ReLU() #nn.Sigmoid()  # Output between 0-1\n",
    "        )\n",
    "        if xavier_norm_init:\n",
    "            nn.init.xavier_uniform_(self.decoder[0].weight)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "    \n",
    "    def kl_sparsity_penalty(self, encoded):\n",
    "        # Penalize the average absolute activation\n",
    "        rho_hat = torch.mean(torch.abs(encoded), dim=0)  # Average absolute activation per hidden unit\n",
    "        #rho_hat = 0.1122\n",
    "        rho = torch.ones_like(rho_hat) * self.sparsity_target  # Target sparsity value\n",
    "        epsilon = 1e-8  # Small value to avoid log(0)\n",
    "\n",
    "        # KL-divergence computation for sparsity\n",
    "        kl_divergence = rho * torch.log(rho / (rho_hat + epsilon)) + (1 - rho) * torch.log((1 - rho) / (1 - rho_hat + epsilon))\n",
    "        kl_divergence = torch.sum(kl_divergence)  # Sum over all hidden units\n",
    "\n",
    "        return self.sparsity_lambda * kl_divergence\n",
    "\n",
    "    # L1-norm sparsity penalty calculation\n",
    "    def l1_sparsity_penalty(self, encoded):\n",
    "        # Compute the mean of absolute values of activations\n",
    "        sparsity_loss = torch.mean(torch.abs(encoded))  # Average absolute activation across all units\n",
    "        #sparsity_loss = 0.1122  # Average absolute activation across all units\n",
    "        return self.sparsity_lambda * sparsity_loss  # Scale by the sparsity weight\n",
    "\n",
    "    # Loss function combining MSE (reconstruction error) and sparsity penalty\n",
    "    def loss_function(self, decoded, original, encoded):\n",
    "        mse_loss = F.mse_loss(decoded, original)  # Mean Squared Error for reconstruction\n",
    "        sparsity_loss = self.l1_sparsity_penalty(encoded)  # Sparsity penalty for hidden layer activations\n",
    "        return mse_loss + sparsity_loss  # Total loss is MSE + sparsity penalty\n",
    "# Instantiate the Sparse Auto-encoder with given dimensions\n",
    "def load_autoencoder_2(sae_path, device):\n",
    "    input_dim = 2048\n",
    "    hidden_dim = 8000\n",
    "    sae_2 = SparseAutoEncoder_2(input_dim, hidden_dim)\n",
    "    sae_2.load_state_dict(torch.load(sae_path, map_location=device))\n",
    "    sae_2 = sae_2.to(device)\n",
    "    # Freeze all parameters of the autoencoder\n",
    "    for param in sae_2.parameters():\n",
    "        param.requires_grad = False\n",
    "    sae_2.eval()\n",
    "    return sae_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Models\n",
    "def get_model_AlexNet(model_path, device):\n",
    "    num_features = 4096\n",
    "    num_classes = 2\n",
    "    model_path = model_path\n",
    "    device = device\n",
    "    \n",
    "    model = alexnet(weights=AlexNet_Weights.DEFAULT) # weights=AlexNet_Weights.DEFAULT) # weights=None\n",
    "    model.classifier[6] = nn.Linear(model.classifier[6].in_features, 2)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model = model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_ResNet(model_path, device):\n",
    "    num_features = 2048\n",
    "    num_classes = 2\n",
    "    model_path = model_path\n",
    "    device = device    \n",
    "    model = models.resnet50(weights=ResNet50_Weights.DEFAULT)   #weights=ResNet50_Weights.DEFAULT)\n",
    "    model.fc = nn.Linear(model.fc.in_features, 2)  # Output for 2 classes\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def activation_correlations(b_val_patch_activations, b_val_no_patch_activations, m_val_patch_activations, m_val_no_patch_activations, act_csv_path):\n",
    "    # Step 1: Print total number of values in tensors and total number of differences\n",
    "    # Number of neurons and images\n",
    "    num_neurons = b_val_patch_activations.shape[1]\n",
    "    #print(f\"Number of val patch neurons: {num_neurons}\")\n",
    "    num_images = b_val_patch_activations.shape[0]\n",
    "    #print(f\"Number of val patch images: {b_val_patch_activations.shape[0]}\")\n",
    "\n",
    "    # Create a binary label vector pp (1 for patch, 0 for no patch)\n",
    "    pp = np.concatenate([np.ones(b_val_patch_activations.shape[0]), np.ones(m_val_patch_activations.shape[0]),\n",
    "                         np.zeros(b_val_no_patch_activations.shape[0]), np.zeros(m_val_no_patch_activations.shape[0])])\n",
    "    \n",
    "    #print(f\"Number of pp labels: {len(pp)}\")\n",
    "    \n",
    "    b_val_patch_activations = b_val_patch_activations.cpu().numpy()\n",
    "    b_val_no_patch_activations = b_val_no_patch_activations.cpu().numpy()\n",
    "    m_val_patch_activations = m_val_patch_activations.cpu().numpy()\n",
    "    m_val_no_patch_activations = m_val_no_patch_activations.cpu().numpy()\n",
    "    # array to store correlations\n",
    "    correlations = np.zeros(num_neurons)  # Shape: (2048,)\n",
    "    for i in range(num_neurons):\n",
    "        # Combine activations for neuron i from both datassets p and np\n",
    "        act_i = np.concatenate([b_val_patch_activations[:, i], m_val_patch_activations[:, i],\n",
    "                                b_val_no_patch_activations[:, i], m_val_no_patch_activations[:, i]])\n",
    "\n",
    "        # Compute correlation between pp and act_i\n",
    "        if np.std(pp) > 0 and np.std(act_i) > 0:\n",
    "            corr = np.corrcoef(pp, act_i)[0, 1]\n",
    "        else:\n",
    "            corr = 0  # Handle constant vectors\n",
    "        #corr_value = np.abs(corr)\n",
    "        correlations[i] = corr\n",
    "        \n",
    "    correlations = np.nan_to_num(correlations)  # Replace NaN values with 0\n",
    "    # Create a DataFrame with neuron indices and their correlations\n",
    "    neuron_data = pd.DataFrame({\n",
    "        \"Neuron_Index\": np.arange(num_neurons),\n",
    "        \"Correlation\": correlations\n",
    "    })\n",
    "    # Sort by correlation in descending order and vsave neurons to csv\n",
    "    neuron_data.sort_values(by=\"Correlation\", ascending=False, inplace=True)\n",
    "    # Save the DataFrame to a CSV file\n",
    "    csv_path = act_csv_path\n",
    "    neuron_data.to_csv(csv_path, index=False)\n",
    "    \n",
    "    return correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def activation_correlations_2(b_val_patch_activations, b_val_no_patch_activations, act_csv_path):\n",
    "    # Step 1: Print total number of values in tensors and total number of differences\n",
    "    # Number of neurons and images\n",
    "    num_neurons = b_val_patch_activations.shape[1]\n",
    "    #print(f\"Number of val patch neurons: {num_neurons}\")\n",
    "    num_images = b_val_patch_activations.shape[0]\n",
    "    #print(f\"Number of val patch images: {b_val_patch_activations.shape[0]}\")\n",
    "\n",
    "    # Create a binary label vector pp (1 for patch, 0 for no patch)\n",
    "    pp = np.concatenate([np.ones(b_val_patch_activations.shape[0]), np.zeros(b_val_no_patch_activations.shape[0])])\n",
    "    \n",
    "    #print(f\"Number of pp labels: {len(pp)}\")\n",
    "    b_val_patch_activations = b_val_patch_activations.cpu().numpy()\n",
    "    b_val_no_patch_activations = b_val_no_patch_activations.cpu().numpy()\n",
    "    \n",
    "    # array to store correlations\n",
    "    correlations = np.zeros(num_neurons)  # Shape: (2048,)\n",
    "    for i in range(num_neurons):\n",
    "        # Combine activations for neuron i from both datassets p and np\n",
    "        act_i = np.concatenate([b_val_patch_activations[:, i], b_val_no_patch_activations[:, i]])\n",
    "        # Compute correlation between pp and act_i\n",
    "        if np.std(pp) > 0 and np.std(act_i) > 0:\n",
    "            corr = np.corrcoef(pp, act_i)[0, 1]\n",
    "        else:\n",
    "            corr = 0  # Handle constant vectors\n",
    "        #corr_value = np.abs(corr)\n",
    "        correlations[i] = corr\n",
    "        \n",
    "    correlations = np.nan_to_num(correlations)  # Replace NaN values with 0\n",
    "    # Create a DataFrame with neuron indices and their correlations\n",
    "    neuron_data = pd.DataFrame({\n",
    "        \"Neuron_Index\": np.arange(num_neurons),\n",
    "        \"Correlation\": correlations\n",
    "    })\n",
    "    # Sort by correlation in descending order and vsave neurons to csv\n",
    "    neuron_data.sort_values(by=\"Correlation\", ascending=False, inplace=True)\n",
    "    # Save the DataFrame to a CSV file\n",
    "    csv_path = act_csv_path\n",
    "    neuron_data.to_csv(csv_path, index=False)\n",
    "    \n",
    "    return correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get activations # 2\n",
    "def get_activations_2(model, feature_extractor, dataloader):\n",
    "    labels_spu = []\n",
    "    labels_all = []\n",
    "    all_activations = []\n",
    "    spu_activations = []\n",
    "    all_class_scores = []\n",
    "    correlations = []\n",
    "    # Extract activations\n",
    "    with torch.no_grad():  # No gradient calculation for inference\n",
    "        for images, labels, benign_malignant, patches in dataloader:\n",
    "            images = images.to(device)  # Send to GPU if available\n",
    "            #images = images.unsqueeze(0)\n",
    "            labels = labels.to(device)\n",
    "            benign_malignant = benign_malignant.to(device)\n",
    "            patches = patches.to(device)\n",
    "            \n",
    "            #if val_loader_name == 'ISIC_testLoader_dataset_malignant':    \n",
    "            for label, patch in zip(benign_malignant, patches):\n",
    "                if patch.item() == 0:\n",
    "                    activations = feature_extractor(images)  # Get activations from avgpool\n",
    "                    activations = activations.view(activations.size(0), -1)  # Flatten avgpool output\n",
    "                    all_activations.append(activations.cpu())  # Collect activations and move to CPU\n",
    "                    labels_all.append(label)\n",
    "                    #class_scores = activations.gather(1, labels.view(-1, 1)).squeeze()\n",
    "                    #all_class_scores.append(class_scores.cpu())\n",
    "                elif patch.item() == 1:\n",
    "                    activations = feature_extractor(images)  # Get activations from avgpool\n",
    "                    activations = activations.view(activations.size(0), -1)  # Flatten avgpool output\n",
    "                    spu_activations.append(activations.cpu())  # Collect activations and move to CPU\n",
    "                    labels_spu.append(label)\n",
    "            \n",
    "    #print(\"all_class_scores\", all_class_scores)\n",
    "    labels_all = torch.tensor(labels_all).to(device)\n",
    "    labels_spu = torch.tensor(labels_spu).to(device)\n",
    "    all_activations = np.vstack(all_activations) # No furtehr needed\n",
    "    all_activations = torch.tensor(all_activations).to(device)\n",
    "    spu_activations = np.vstack(spu_activations) # No furtehr needed\n",
    "    spu_activations = torch.tensor(spu_activations).to(device)\n",
    "    return all_activations, labels_all, spu_activations, labels_spu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get activations\n",
    "def get_activations_AlexNet(model, dataloader, device):\n",
    "    model.eval()\n",
    "    activations = []\n",
    "    def hook_fn(module, input, output):\n",
    "        activations.append(output.detach().cpu())\n",
    "    handle = model.classifier[4].register_forward_hook(hook_fn)  # fc2 linear layer (pre-ReLU)\n",
    "    with torch.no_grad():\n",
    "        for images, _, _, _ in dataloader:\n",
    "            _ = model(images.to(device))\n",
    "    handle.remove()\n",
    "    act_tensors = torch.cat(activations, dim=0).squeeze() \n",
    "    \n",
    "    return act_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations_ResNet(model, dataloader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    activations = []\n",
    "    def hook_fn(module, input, output):\n",
    "        activations.append(output.detach().cpu())\n",
    "    handle = model.avgpool.register_forward_hook(hook_fn)\n",
    "    with torch.no_grad():\n",
    "        for images, labels, _, _ in dataloader:\n",
    "            images = images.to(device)\n",
    "            _ = model(images)\n",
    "    handle.remove()\n",
    "    act_tensor = torch.cat(activations, dim=0).squeeze()  # shape: (N, 2048)\n",
    "    return act_tensor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load top neurons from CSV based on a percentage\n",
    "def load_top_neurons_from_csv(csv_path, percentage):\n",
    "    \"\"\"\n",
    "    Load top neurons based on the specified percentage from the saved CSV file.\n",
    "    \"\"\"\n",
    "    neuron_data = pd.read_csv(csv_path)\n",
    "\n",
    "    # Calculate the number of top neurons to select\n",
    "    top_count = int(len(neuron_data) * (percentage / 100))\n",
    "\n",
    "    # Select the top neurons based on their correlation difference\n",
    "    top_neurons = neuron_data.iloc[:top_count][\"Neuron_Index\"].values\n",
    "\n",
    "    # Debugging for 0% muting\n",
    "    if percentage == 0:\n",
    "        assert len(top_neurons) == 0, \"Top neurons list should be empty for 0% muting.\"\n",
    "\n",
    "    #print(f\"Loaded top {percentage}% neurons ({top_count} neurons) for muting.\")\n",
    "    return top_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-group accuracy\n",
    "def calculate_group_accuracy(predictions, true_labels):\n",
    "    return accuracy_score(predictions, true_labels)\n",
    "\n",
    "def classify_with_RestNet(model, all_activations):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    pruned_predictions = []\n",
    "    #with torch.enable_grad():\n",
    "    for act  in all_activations:\n",
    "        activation_tensor = act.to(device)\n",
    "        output = model.fc(activation_tensor)\n",
    "        prediction = torch.argmax(torch.nn.functional.softmax(output, dim=0)).item()\n",
    "        pruned_predictions.append(prediction)\n",
    "    return pruned_predictions\n",
    "\n",
    "def classify_with_AlexNet(model, all_activations):\n",
    "    model.eval() \n",
    "    pruned_predictions = []\n",
    "    for activation in all_activations:\n",
    "        # Convert numpy activation to tensor\n",
    "        activation_tensor = activation.to(device)\n",
    "        #activation_tensor = torch.from_numpy(activation).float().to(device)\n",
    "        relu_output = model.classifier[5](activation_tensor)  # Apply ReLU\n",
    "        output = model.classifier[6](relu_output)  # Apply fc3\n",
    "        prediction = torch.argmax(output).item()\n",
    "        #prediction = torch.argmax(torch.nn.functional.softmax(output, dim=0)).item()\n",
    "        pruned_predictions.append(prediction)\n",
    "    return pruned_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project activations into sparse space\n",
    "def project_to_sae(sae, all_activations, device):\n",
    "    #sae.to(device)\n",
    "    with torch.no_grad():\n",
    "        projected = sae.encoder(torch.from_numpy(all_activations).float().to(device))\n",
    "    return projected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_path = 'ResNet_ISIC_seed-1.pth'\n",
    "sae_path = 'ResNet-50-full-model-without-relu-SAE_100_epochs.pth'\n",
    "sparse_act_csv_path = 'ResNet-on-ISIC_activations.csv'\n",
    "Avg_group_acc = []\n",
    "Avg_worst_group_acc = []\n",
    "Acc_benign_core = []\n",
    "Acc_benign_spu = []\n",
    "Acc_malignant_core = []\n",
    "Acc_malignant_spu = []\n",
    "x = [0.2, 0.4, 0.6, 0.8, 1.0, 2.0, 4.0, 8.0]\n",
    "# Get model and feature_extractor\n",
    "model = get_model_ResNet(model_path, device)\n",
    "\n",
    "benign_val_loader_p = ISIC_valLoader_benign_patch_200\n",
    "benign_val_loader_np = ISIC_valLoader_benign_no_patch_200\n",
    "malignant_val_loader_p = ISIC_valLoader_malignant_patch_200\n",
    "malignant_val_loader_np = ISIC_valLoader_malignant_no_patch_200\n",
    "\n",
    "test_benign_loader_p = ISIC_testLoader_dataset_benign_patch \n",
    "test_benign_loader_np = ISIC_testLoader_dataset_benign_no_patch \n",
    "test_malignant_loader_p = ISIC_testLoader_dataset_malignant_patch\n",
    "test_malignant_loader_np = ISIC_testLoader_dataset_malignant_no_patch\n",
    "\n",
    "# Get activations \n",
    "b_val_activations_no_patch  = get_activations_ResNet(model, benign_val_loader_np, device)\n",
    "b_val_activations_patch  = get_activations_ResNet(model, benign_val_loader_p, device)\n",
    "m_val_activations_no_patch  = get_activations_ResNet(model, malignant_val_loader_np, device)\n",
    "m_val_activations_patch  = get_activations_ResNet(model, malignant_val_loader_p, device)\n",
    "\n",
    "test_benign_no_patch  = get_activations_ResNet(model, test_benign_loader_np, device)\n",
    "test_benign_patch  = get_activations_ResNet(model, test_benign_loader_p, device)\n",
    "test_malignant_no_patch  = get_activations_ResNet(model, test_malignant_loader_np, device)\n",
    "test_malignant_patch  = get_activations_ResNet(model, test_malignant_loader_p, device)\n",
    "print(\"classify before sparse muting.....\")\n",
    "b_val_activations_patch_pred = classify_with_RestNet(model, b_val_activations_patch)\n",
    "b_val_activations_no_patch_pred = classify_with_RestNet(model, b_val_activations_no_patch)\n",
    "test_benign_patch_pred = classify_with_RestNet(model, test_benign_patch)\n",
    "test_benign_no_patch_pred = classify_with_RestNet(model, test_benign_no_patch)\n",
    "test_malignant_patch_pred = classify_with_RestNet(model, test_malignant_patch)\n",
    "test_malignant_no_patch_pred = classify_with_RestNet(model, test_malignant_no_patch)\n",
    "# Calculate group accuracy\n",
    "b_val_activations_patch_acc = calculate_group_accuracy(b_val_activations_patch_pred, [0] * len(b_val_activations_patch_pred))\n",
    "b_val_activations_no_patch_acc = calculate_group_accuracy(b_val_activations_no_patch_pred, [0] * len(b_val_activations_no_patch_pred))\n",
    "test_benign_patch_acc = calculate_group_accuracy(test_benign_patch_pred, [0] * len(test_benign_patch_pred))\n",
    "test_benign_no_patch_acc = calculate_group_accuracy(test_benign_no_patch_pred, [0] * len(test_benign_no_patch_pred))\n",
    "test_malignant_patch_acc = calculate_group_accuracy(test_malignant_patch_pred, [1] * len(test_malignant_patch_pred))\n",
    "test_malignant_no_patch_acc = calculate_group_accuracy(test_malignant_no_patch_pred, [1] * len(test_malignant_no_patch_pred))\n",
    "print(\"Accuracy for Val. Benign_P(Spu): \", b_val_activations_patch_acc)\n",
    "print(\"Accuracy for Va. Benign_NP(Core): \", b_val_activations_no_patch_acc)\n",
    "\n",
    "print(\"Accuracy for Benign_NP(Core): \", test_benign_no_patch_acc)\n",
    "print(\"Accuracy for Benign_P(Spu): \", test_benign_patch_acc)\n",
    "print(\"Accuracy for Malignant_NP(Core): \", test_malignant_no_patch_acc)\n",
    "print(\"Accuracy for Malignant_P(Spu): \", test_malignant_patch_acc)\n",
    "print(\" End of classification before sparse muting......\")\n",
    "\n",
    "# The sparse space: Project activations\n",
    "sae = load_autoencoder_2(sae_path, device)\n",
    "sae.to(device)\n",
    "b_val_activations_patch = b_val_activations_patch.cpu().numpy()\n",
    "b_val_activations_no_patch = b_val_activations_no_patch.cpu().numpy()\n",
    "m_val_activations_patch = m_val_activations_patch.cpu().numpy()\n",
    "m_val_activations_no_patch =  m_val_activations_no_patch.cpu().numpy()\n",
    "test_benign_patch = test_benign_patch.cpu().numpy()\n",
    "test_benign_no_patch = test_benign_no_patch.cpu().numpy()\n",
    "test_malignant_patch = test_malignant_patch.cpu().numpy()\n",
    "test_malignant_no_patch = test_malignant_no_patch.cpu().numpy()\n",
    "\n",
    "b_projected_val_patch = project_to_sae(sae, b_val_activations_patch, device)\n",
    "b_projected_val_no_patch = project_to_sae(sae, b_val_activations_no_patch, device)\n",
    "m_projected_val_patch = project_to_sae(sae, m_val_activations_patch, device)\n",
    "m_projected_val_no_patch = project_to_sae(sae, m_val_activations_no_patch, device)\n",
    "projected_benign_test_patch = project_to_sae(sae, test_benign_patch, device)\n",
    "projected_benign_test_no_patch = project_to_sae(sae, test_benign_no_patch, device)\n",
    "projected_malignant_test_patch = project_to_sae(sae, test_malignant_patch, device)\n",
    "projected_malignant_test_no_patch = project_to_sae(sae, test_malignant_no_patch, device)\n",
    "\n",
    "# corr. \n",
    "correlations = activation_correlations_2(b_projected_val_patch, b_projected_val_no_patch, sparse_act_csv_path)\n",
    "for percentage in x:\n",
    "    # Correlaiton based Activations\n",
    "    top_neurons = load_top_neurons_from_csv(sparse_act_csv_path, percentage=percentage)\n",
    "    # Muting neurons\n",
    "    b_projected_val_patch_muted = b_projected_val_patch.clone().detach()\n",
    "    b_projected_val_no_patch_muted = b_projected_val_no_patch.clone().detach()\n",
    "    projected_malignant_test_patch_muted = projected_malignant_test_patch.clone().detach()\n",
    "    projected_malignant_test_no_patch_muted = projected_malignant_test_no_patch.clone().detach()\n",
    "    projected_benign_test_patch_muted = projected_benign_test_patch.clone().detach()\n",
    "    projected_benign_test_no_patch_muted = projected_benign_test_no_patch.clone().detach()\n",
    "    b_projected_val_patch_muted[:, top_neurons] = 0\n",
    "    b_projected_val_no_patch_muted[:, top_neurons] = 0\n",
    "    projected_malignant_test_patch_muted[:, top_neurons] = 0\n",
    "    projected_malignant_test_no_patch_muted[:, top_neurons] = 0\n",
    "    projected_benign_test_patch_muted[:, top_neurons] = 0\n",
    "    projected_benign_test_no_patch_muted[:, top_neurons] = 0\n",
    "    # Decode\n",
    "    b_decoded_val_patch = sae.decoder(b_projected_val_patch_muted).to(device)\n",
    "    b_decoded_val_no_patch = sae.decoder(b_projected_val_no_patch_muted).to(device)\n",
    "    decoded_malignant_test_patch = sae.decoder(projected_malignant_test_patch_muted).to(device)\n",
    "    decoded_malignant_test_no_patch = sae.decoder(projected_malignant_test_no_patch_muted).to(device)\n",
    "    decoded_benign_test_patch = sae.decoder(projected_benign_test_patch_muted).to(device)\n",
    "    decoded_benign_test_no_patch = sae.decoder(projected_benign_test_no_patch_muted).to(device)\n",
    "    # Classify\n",
    "    b_predictions_val_patch_after = classify_with_RestNet(model, b_decoded_val_patch)\n",
    "    b_predictions_val_no_patch_after = classify_with_RestNet(model, b_decoded_val_no_patch)\n",
    "    predictions_test_malignant_patch_after = classify_with_RestNet(model, decoded_malignant_test_patch)\n",
    "    predictions_test_malignant_no_patch_after = classify_with_RestNet(model, decoded_malignant_test_no_patch)\n",
    "    predictions_test_benign_patch_after = classify_with_RestNet(model, decoded_benign_test_patch)\n",
    "    predictions_test_benign_no_patch_after = classify_with_RestNet(model, decoded_benign_test_no_patch)\n",
    "    # Calculate group accuracy\n",
    "    b_accuracy_val_patch_after = calculate_group_accuracy(b_predictions_val_patch_after, [0] * len(b_predictions_val_patch_after))\n",
    "    b_accuracy_val_no_patch_after = calculate_group_accuracy(b_predictions_val_no_patch_after, [0] * len(b_predictions_val_no_patch_after))\n",
    "    accuracy_test_malignant_patch_after = calculate_group_accuracy(predictions_test_malignant_patch_after, [1] * len(predictions_test_malignant_patch_after))\n",
    "    accuracy_test_malignant_no_patch_after = calculate_group_accuracy(predictions_test_malignant_no_patch_after, [1] * len(predictions_test_malignant_no_patch_after))\n",
    "    accuracy_test_benign_patch_after = calculate_group_accuracy(predictions_test_benign_patch_after, [0] * len(predictions_test_benign_patch_after))\n",
    "    accuracy_test_benign_no_patch_after = calculate_group_accuracy(predictions_test_benign_no_patch_after, [0] * len(predictions_test_benign_no_patch_after))\n",
    "    # Worst and average group accuracies\n",
    "    AGA = (accuracy_test_malignant_patch_after + accuracy_test_malignant_no_patch_after + accuracy_test_benign_patch_after + accuracy_test_benign_no_patch_after) / 4\n",
    "    AWGA = min(accuracy_test_malignant_patch_after, accuracy_test_malignant_no_patch_after, accuracy_test_benign_patch_after, accuracy_test_benign_no_patch_after)\n",
    "    Avg_group_acc.append(AGA)\n",
    "    Avg_worst_group_acc.append(AWGA)\n",
    "    Acc_benign_core.append(accuracy_test_benign_no_patch_after)\n",
    "    Acc_benign_spu.append(accuracy_test_benign_patch_after)\n",
    "    Acc_malignant_core.append(accuracy_test_malignant_no_patch_after)\n",
    "    Acc_malignant_spu.append(accuracy_test_malignant_patch_after)\n",
    "    # Rounded values\n",
    "    Avg_group_acc_rv = [round(x * 100, 2) for x in Avg_group_acc]\n",
    "    Avg_worst_group_acc_rv = [round(x * 100, 2) for x in Avg_worst_group_acc]\n",
    "    Acc_benign_core_rv = [round(x * 100, 2) for x in Acc_benign_core]\n",
    "    Acc_benign_spu_rv = [round(x * 100, 2) for x in Acc_benign_spu]\n",
    "    Acc_malignant_core_rv = [round(x * 100, 2) for x in Acc_malignant_core]\n",
    "    Acc_malignant_spu_rv = [round(x * 100, 2) for x in Acc_malignant_spu]\n",
    "    print(\"Muting percentage x = \", percentage)\n",
    "print(\"*\" * 50)\n",
    "print(\"We apply muting percentage: x = \", x)\n",
    "print(f\"Avg_group_acc: {Avg_group_acc_rv}\")\n",
    "print(f\"Avg_worst_group_acc: {Avg_worst_group_acc_rv}\")  \n",
    "print(f\"Acc_benign_core: {Acc_benign_core_rv}\")\n",
    "print(f\"Acc_benign_spu: {Acc_benign_spu_rv}\")\n",
    "print(f\"Acc_malignant_core: {Acc_malignant_core_rv}\")\n",
    "print(f\"Acc_malignant_spu: {Acc_malignant_spu_rv}\")\n",
    "\n",
    "    #print(\"Prediction and Evaluation All Groups:\")\n",
    "    #prediction_and_evaluation(model, val_all_activations_decoded, val_loader, b_val_labels_all)\n",
    "print(\"*\" * 50)\n",
    "print(\"Evalutaion Complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xAI-bio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
