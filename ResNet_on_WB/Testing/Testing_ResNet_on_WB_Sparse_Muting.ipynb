{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from torchvision.models import alexnet, AlexNet_Weights, resnet50, ResNet50_Weights, resnet18, ResNet18_Weights, resnet101, ResNet101_Weights, VGG19_Weights, vgg19\n",
    "from PIL import *\n",
    "import PIL.Image\n",
    "import gc\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(23)\n",
    "np.random.seed(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define Datasets and Dataloader\n",
    "class WaterbirdsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.metadata = pd.read_csv(csv_file)  # Metadata file with bird type and background\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = f\"{self.root_dir}/{self.metadata.iloc[idx]['img_filename']}\"\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        bird_type = self.metadata.iloc[idx]['y']  # Waterbird=1, Landbird=0\n",
    "        background = self.metadata.iloc[idx]['place']  # Water=1, Land=0\n",
    "        label = bird_type  # For training, we only care about bird type\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label, bird_type, background\n",
    "\n",
    "# 2. Transforms and Data Preparation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load training and testing datasets\n",
    "train_dataset = WaterbirdsDataset(\n",
    "    csv_file='/run/determined/workdir/SCLearning_WB/split-metadata/output_metadata/train_metadata_updated.csv',\n",
    "    root_dir='/run/determined/workdir/SCLearning_WB/WB_DB/all_images_DB/train_DB/all_birds_train',\n",
    "    transform=transform\n",
    ")\n",
    "test_dataset = WaterbirdsDataset(\n",
    "    csv_file='/run/determined/workdir/SCLearning_WB/split-metadata/output_metadata/test_metadata_updated.csv',\n",
    "    root_dir='/run/determined/workdir/SCLearning_WB/WB_DB/all_images_DB/test_DB/all_birds_test',\n",
    "    transform=transform\n",
    ")\n",
    "test_dataset_LB = WaterbirdsDataset(\n",
    "    csv_file='/run/determined/workdir/SCLearning_WB/split-metadata/output_metadata/test_metadata_updated_LB.csv',\n",
    "    root_dir='/run/determined/workdir/SCLearning_WB/WB_DB/all_images_DB/test_DB/all_birds_test',\n",
    "    transform=transform\n",
    ")\n",
    "test_dataset_LB_no_patch = WaterbirdsDataset(\n",
    "    csv_file='/run/determined/workdir/SCLearning_WB/split-metadata/output_metadata/test_metadata_updated_LB_no_patch.csv',\n",
    "    root_dir='/run/determined/workdir/SCLearning_WB/WB_DB/all_images_DB/test_DB/all_birds_test',\n",
    "    transform=transform\n",
    ")\n",
    "test_dataset_LB_patch = WaterbirdsDataset(\n",
    "    csv_file='/run/determined/workdir/SCLearning_WB/split-metadata/output_metadata/test_metadata_updated_LB_patch.csv',\n",
    "    root_dir='/run/determined/workdir/SCLearning_WB/WB_DB/all_images_DB/test_DB/all_birds_test',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset_LB_25 = WaterbirdsDataset(\n",
    "    csv_file='/run/determined/workdir/SCLearning_WB/split-metadata/output_metadata/test_metadata_updated_LB_25.csv',\n",
    "    root_dir='/run/determined/workdir/SCLearning_WB/WB_DB/all_images_DB/test_DB/all_birds_test',\n",
    "    transform=transform\n",
    ")\n",
    "test_dataset_WB = WaterbirdsDataset(\n",
    "    csv_file='/run/determined/workdir/SCLearning_WB/split-metadata/output_metadata/test_metadata_updated_WB.csv',\n",
    "    root_dir='/run/determined/workdir/SCLearning_WB/WB_DB/all_images_DB/test_DB/all_birds_test',\n",
    "    transform=transform\n",
    ")\n",
    "test_dataset_WB_no_patch = WaterbirdsDataset(\n",
    "    csv_file='/run/determined/workdir/SCLearning_WB/split-metadata/output_metadata/test_metadata_updated_WB_no_patch.csv',\n",
    "    root_dir='/run/determined/workdir/SCLearning_WB/WB_DB/all_images_DB/test_DB/all_birds_test',\n",
    "    transform=transform\n",
    ")\n",
    "test_dataset_WB_patch = WaterbirdsDataset(\n",
    "    csv_file='/run/determined/workdir/SCLearning_WB/split-metadata/output_metadata/test_metadata_updated_WB_patch.csv',\n",
    "    root_dir='/run/determined/workdir/SCLearning_WB/WB_DB/all_images_DB/test_DB/all_birds_test',\n",
    "    transform=transform\n",
    ") \n",
    "test_dataset_WB_25 = WaterbirdsDataset(\n",
    "    csv_file='/run/determined/workdir/SCLearning_WB/split-metadata/output_metadata/test_metadata_updated_WB_25.csv',\n",
    "    root_dir='/run/determined/workdir/SCLearning_WB/WB_DB/all_images_DB/test_DB/all_birds_test',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "val_dataset_LB = WaterbirdsDataset(\n",
    "    #csv_file='/run/determined/workdir/SCLearning_WB/split-metadata/output_metadata/val_metadata_updated_LB.csv',\n",
    "    csv_file='/run/determined/workdir/SCLearning_WB/split-metadata/output_metadata/val_metadata_0_1_LB.csv',\n",
    "    root_dir='/run/determined/workdir/SCLearning_WB/WB_DB/all_images_DB',\n",
    "    transform=transform\n",
    ")\n",
    "val_dataset_WB = WaterbirdsDataset(\n",
    "    csv_file='/run/determined/workdir/SCLearning_WB/split-metadata/output_metadata/val_metadata_updated_WB.csv',\n",
    "    root_dir='/run/determined/workdir/SCLearning_WB/WB_DB/all_images_DB',\n",
    "    transform=transform\n",
    ")\n",
    "val_dataset_WB_LB = WaterbirdsDataset(\n",
    "    csv_file='/run/determined/workdir/SCLearning_WB/split-metadata/output_metadata/val_metadata_WB+LB.csv',\n",
    "    root_dir='/run/determined/workdir/SCLearning_WB/WB_DB/all_images_DB',\n",
    "    transform=transform\n",
    ")\n",
    "# Variational DB\n",
    "val_dataset_LB_25 = WaterbirdsDataset(\n",
    "    csv_file='/run/determined/workdir/SCLearning_WB/split-metadata/output_metadata/val_metadata_updated_LB_yes_no_25.csv',\n",
    "    #csv_file='/run/determined/workdir/SCLearning_WB/split-metadata/output_metadata/val_metadata_0_1_LB.csv',\n",
    "    root_dir='/run/determined/workdir/SCLearning_WB/WB_DB/all_images_DB',\n",
    "    transform=transform\n",
    ")\n",
    "val_dataset_LB_no_patch_25 = WaterbirdsDataset(\n",
    "    csv_file='/run/determined/workdir/SCLearning_WB/split-metadata/output_metadata/val_metadata_updated_LB_no_patch_25.csv',\n",
    "    #csv_file='/run/determined/workdir/SCLearning_WB/split-metadata/output_metadata/val_metadata_0_1_LB.csv',\n",
    "    root_dir='/run/determined/workdir/SCLearning_WB/WB_DB/all_images_DB',\n",
    "    transform=transform\n",
    ")\n",
    "val_dataset_LB_patch_25 = WaterbirdsDataset(\n",
    "    csv_file='/run/determined/workdir/SCLearning_WB/split-metadata/output_metadata/val_metadata_updated_LB_patch_25.csv',\n",
    "    #csv_file='/run/determined/workdir/SCLearning_WB/split-metadata/output_metadata/val_metadata_0_1_LB.csv',\n",
    "    root_dir='/run/determined/workdir/SCLearning_WB/WB_DB/all_images_DB',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "val_dataset_WB_25 = WaterbirdsDataset(\n",
    "    csv_file='/run/determined/workdir/SCLearning_WB/split-metadata/output_metadata/val_metadata_updated_WB_yes_no_25.csv',\n",
    "    root_dir='/run/determined/workdir/SCLearning_WB/WB_DB/all_images_DB',\n",
    "    transform=transform\n",
    ")\n",
    "val_dataset_WB_no_patch_25 = WaterbirdsDataset(\n",
    "    csv_file='/run/determined/workdir/SCLearning_WB/split-metadata/output_metadata/val_metadata_updated_WB_no_patch_25.csv',\n",
    "    root_dir='/run/determined/workdir/SCLearning_WB/WB_DB/all_images_DB',\n",
    "    transform=transform\n",
    ")\n",
    "val_dataset_WB_patch_25 = WaterbirdsDataset(\n",
    "    csv_file='/run/determined/workdir/SCLearning_WB/split-metadata/output_metadata/val_metadata_updated_WB_patch_25.csv',\n",
    "    root_dir='/run/determined/workdir/SCLearning_WB/WB_DB/all_images_DB',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "val_dataset_LB_50 = WaterbirdsDataset(\n",
    "    csv_file='/run/determined/workdir/SCLearning_WB/split-metadata/output_metadata/val_metadata_updated_LB_yes_no_50.csv',\n",
    "    #csv_file='/run/determined/workdir/SCLearning_WB/split-metadata/output_metadata/val_metadata_0_1_LB.csv',\n",
    "    root_dir='/run/determined/workdir/SCLearning_WB/WB_DB/all_images_DB',\n",
    "    transform=transform\n",
    ")\n",
    "val_dataset_LB_no_patch_50 = WaterbirdsDataset(\n",
    "    csv_file='/run/determined/workdir/SCLearning_WB/split-metadata/output_metadata/val_metadata_updated_LB_no_patch_50.csv',\n",
    "    #csv_file='/run/determined/workdir/SCLearning_WB/split-metadata/output_metadata/val_metadata_0_1_LB.csv',\n",
    "    root_dir='/run/determined/workdir/SCLearning_WB/WB_DB/all_images_DB',\n",
    "    transform=transform\n",
    ")\n",
    "val_dataset_LB_patch_50 = WaterbirdsDataset(\n",
    "    csv_file='/run/determined/workdir/SCLearning_WB/split-metadata/output_metadata/val_metadata_updated_LB_patch_50.csv',\n",
    "    #csv_file='/run/determined/workdir/SCLearning_WB/split-metadata/output_metadata/val_metadata_0_1_LB.csv',\n",
    "    root_dir='/run/determined/workdir/SCLearning_WB/WB_DB/all_images_DB',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "val_dataset_WB_50 = WaterbirdsDataset(\n",
    "    csv_file='/run/determined/workdir/SCLearning_WB/split-metadata/output_metadata/val_metadata_updated_WB_yes_no_50.csv',\n",
    "    root_dir='/run/determined/workdir/SCLearning_WB/WB_DB/all_images_DB',\n",
    "    transform=transform\n",
    ")\n",
    "val_dataset_WB_no_patch_50 = WaterbirdsDataset(\n",
    "    csv_file='/run/determined/workdir/SCLearning_WB/split-metadata/output_metadata/val_metadata_updated_WB_no_patch_50.csv',\n",
    "    root_dir='/run/determined/workdir/SCLearning_WB/WB_DB/all_images_DB',\n",
    "    transform=transform\n",
    ")\n",
    "val_dataset_WB_patch_50 = WaterbirdsDataset(\n",
    "    csv_file='/run/determined/workdir/SCLearning_WB/split-metadata/output_metadata/val_metadata_updated_WB_patch_50.csv',\n",
    "    root_dir='/run/determined/workdir/SCLearning_WB/WB_DB/all_images_DB',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "\n",
    "val_dataset_LB_100 = WaterbirdsDataset(\n",
    "    csv_file='/run/determined/workdir/SCLearning_WB/split-metadata/output_metadata/val_metadata_updated_LB_yes_no_100.csv',\n",
    "    #csv_file='/run/determined/workdir/SCLearning_WB/split-metadata/output_metadata/val_metadata_0_1_LB.csv',\n",
    "    root_dir='/run/determined/workdir/SCLearning_WB/WB_DB/all_images_DB',\n",
    "    transform=transform\n",
    ")\n",
    "val_dataset_LB_patch_100 = WaterbirdsDataset(\n",
    "    csv_file='/run/determined/workdir/SCLearning_WB/split-metadata/output_metadata/val_metadata_updated_LB_patch_100.csv',\n",
    "    #csv_file='/run/determined/workdir/SCLearning_WB/split-metadata/output_metadata/val_metadata_0_1_LB.csv',\n",
    "    root_dir='/run/determined/workdir/SCLearning_WB/WB_DB/all_images_DB',\n",
    "    transform=transform\n",
    ")\n",
    "val_dataset_LB_no_patch_100 = WaterbirdsDataset(\n",
    "    csv_file='/run/determined/workdir/SCLearning_WB/split-metadata/output_metadata/val_metadata_updated_LB_no_patch_100.csv',\n",
    "    #csv_file='/run/determined/workdir/SCLearning_WB/split-metadata/output_metadata/val_metadata_0_1_LB.csv',\n",
    "    root_dir='/run/determined/workdir/SCLearning_WB/WB_DB/all_images_DB',\n",
    "    transform=transform\n",
    ")\n",
    "val_dataset_WB_100 = WaterbirdsDataset(\n",
    "    csv_file='/run/determined/workdir/SCLearning_WB/split-metadata/output_metadata/val_metadata_updated_WB_yes_no_100.csv',\n",
    "    root_dir='/run/determined/workdir/SCLearning_WB/WB_DB/all_images_DB',\n",
    "    transform=transform\n",
    ")\n",
    "val_dataset_WB_patch_100 = WaterbirdsDataset(\n",
    "    csv_file='/run/determined/workdir/SCLearning_WB/split-metadata/output_metadata/val_metadata_updated_WB_patch_100.csv',\n",
    "    root_dir='/run/determined/workdir/SCLearning_WB/WB_DB/all_images_DB',\n",
    "    transform=transform\n",
    ")\n",
    "val_dataset_WB_no_patch_100 = WaterbirdsDataset(\n",
    "    csv_file='/run/determined/workdir/SCLearning_WB/split-metadata/output_metadata/val_metadata_updated_WB_no_patch_100.csv',\n",
    "    root_dir='/run/determined/workdir/SCLearning_WB/WB_DB/all_images_DB',\n",
    "    transform=transform\n",
    ")\n",
    "val_dataset_LB_200 = WaterbirdsDataset(\n",
    "    csv_file='/run/determined/workdir/SCLearning_WB/split-metadata/output_metadata/val_metadata_updated_LB_yes_no_200.csv',\n",
    "    #csv_file='/run/determined/workdir/SCLearning_WB/split-metadata/output_metadata/val_metadata_0_1_LB.csv',\n",
    "    root_dir='/run/determined/workdir/SCLearning_WB/WB_DB/all_images_DB',\n",
    "    transform=transform\n",
    ")\n",
    "val_dataset_LB_patch_200 = WaterbirdsDataset(\n",
    "    csv_file='/run/determined/workdir/SCLearning_WB/split-metadata/output_metadata/val_metadata_updated_LB_patch_200.csv',\n",
    "    #csv_file='/run/determined/workdir/SCLearning_WB/split-metadata/output_metadata/val_metadata_0_1_LB.csv',\n",
    "    root_dir='/run/determined/workdir/SCLearning_WB/WB_DB/all_images_DB',\n",
    "    transform=transform\n",
    ")\n",
    "val_dataset_LB_no_patch_200 = WaterbirdsDataset(\n",
    "    csv_file='/run/determined/workdir/SCLearning_WB/split-metadata/output_metadata/val_metadata_updated_LB_no_patch_200.csv',\n",
    "    #csv_file='/run/determined/workdir/SCLearning_WB/split-metadata/output_metadata/val_metadata_0_1_LB.csv',\n",
    "    root_dir='/run/determined/workdir/SCLearning_WB/WB_DB/all_images_DB',\n",
    "    transform=transform\n",
    ")\n",
    "val_dataset_WB_200 = WaterbirdsDataset(\n",
    "    csv_file='/run/determined/workdir/SCLearning_WB/split-metadata/output_metadata/val_metadata_updated_WB_yes_no_200.csv',\n",
    "    root_dir='/run/determined/workdir/SCLearning_WB/WB_DB/all_images_DB',\n",
    "    transform=transform\n",
    ")\n",
    "val_dataset_WB_patch_200 = WaterbirdsDataset(\n",
    "    csv_file='/run/determined/workdir/SCLearning_WB/split-metadata/output_metadata/val_metadata_updated_WB_patch_200.csv',\n",
    "    root_dir='/run/determined/workdir/SCLearning_WB/WB_DB/all_images_DB',\n",
    "    transform=transform\n",
    ")\n",
    "val_dataset_WB_no_patch_200 = WaterbirdsDataset(\n",
    "    csv_file='/run/determined/workdir/SCLearning_WB/split-metadata/output_metadata/val_metadata_updated_WB_no_patch_200.csv',\n",
    "    root_dir='/run/determined/workdir/SCLearning_WB/WB_DB/all_images_DB',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "batch_size = 64\n",
    "val_loader_LB_25 = DataLoader(val_dataset_LB_25, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "val_loader_WB_25 = DataLoader(val_dataset_WB_25, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "val_loader_LB_50 = DataLoader(val_dataset_LB_50, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "val_loader_WB_50 = DataLoader(val_dataset_WB_50, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "val_loader_LB_100 = DataLoader(val_dataset_LB_100, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "val_loader_WB_100 = DataLoader(val_dataset_WB_100, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "val_loader_LB_200 = DataLoader(val_dataset_LB_200, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "val_loader_WB_200 = DataLoader(val_dataset_WB_200, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader_LB = DataLoader(test_dataset_LB, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader_LB_25 = DataLoader(test_dataset_LB_25, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader_WB = DataLoader(test_dataset_WB, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader_WB_25 = DataLoader(test_dataset_WB_25, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "test_Loader_LB_no_patch = DataLoader(test_dataset_LB_no_patch, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_Loader_LB_patch = DataLoader(test_dataset_LB_patch, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_Loader_WB_no_patch = DataLoader(test_dataset_WB_no_patch, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_Loader_WB_patch = DataLoader(test_dataset_WB_patch, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "val_Loader_LB_no_patch_200 = DataLoader(val_dataset_LB_no_patch_200, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_Loader_LB_patch_200 = DataLoader(val_dataset_LB_patch_200, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_Loader_WB_no_patch_200 = DataLoader(val_dataset_WB_no_patch_200, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_Loader_WB_patch_200 = DataLoader(val_dataset_WB_patch_200, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "val_Loader_LB_no_patch_100 = DataLoader(val_dataset_LB_no_patch_100, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_Loader_LB_patch_100 = DataLoader(val_dataset_LB_patch_100, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_Loader_WB_no_patch_100 = DataLoader(val_dataset_WB_no_patch_100, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_Loader_WB_patch_100 = DataLoader(val_dataset_WB_patch_100, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "val_Loader_LB_no_patch_50 = DataLoader(val_dataset_LB_no_patch_50, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_Loader_LB_patch_50 = DataLoader(val_dataset_LB_patch_50, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_Loader_WB_no_patch_50 = DataLoader(val_dataset_WB_no_patch_50, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_Loader_WB_patch_50 = DataLoader(val_dataset_WB_patch_50, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "val_Loader_LB_no_patch_25 = DataLoader(val_dataset_LB_no_patch_25, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_Loader_LB_patch_25 = DataLoader(val_dataset_LB_patch_25, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_Loader_WB_no_patch_25 = DataLoader(val_dataset_WB_no_patch_25, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_Loader_WB_patch_25 = DataLoader(val_dataset_WB_patch_25, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "val_loader_LB = DataLoader(val_dataset_LB, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "val_loader_WB = DataLoader(val_dataset_WB, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "val_loader_WB_LB = DataLoader(val_dataset_WB_LB, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseAutoEncoder_2(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, sparsity_lambda=0.7, xavier_norm_init=True):\n",
    "        super(SparseAutoEncoder_2, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.sparsity_lambda = sparsity_lambda\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, self.hidden_dim), #nn.BatchNorm1d(hidden_dim)\n",
    "            nn.GroupNorm(num_groups=16, num_channels=hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        if xavier_norm_init:\n",
    "            nn.init.xavier_uniform_(self.encoder[0].weight)  # Xavier initialization\n",
    "            \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dim, self.input_dim),\n",
    "            #nn.ReLU() #nn.Sigmoid()  # Output between 0-1\n",
    "        )\n",
    "        if xavier_norm_init:\n",
    "            nn.init.xavier_uniform_(self.decoder[0].weight)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "    \n",
    "    def kl_sparsity_penalty(self, encoded):\n",
    "        # Penalize the average absolute activation\n",
    "        rho_hat = torch.mean(torch.abs(encoded), dim=0)  # Average absolute activation per hidden unit\n",
    "        #rho_hat = 0.1122\n",
    "        rho = torch.ones_like(rho_hat) * self.sparsity_target  # Target sparsity value\n",
    "        epsilon = 1e-8  # Small value to avoid log(0)\n",
    "\n",
    "        # KL-divergence computation for sparsity\n",
    "        kl_divergence = rho * torch.log(rho / (rho_hat + epsilon)) + (1 - rho) * torch.log((1 - rho) / (1 - rho_hat + epsilon))\n",
    "        kl_divergence = torch.sum(kl_divergence)  # Sum over all hidden units\n",
    "\n",
    "        return self.sparsity_lambda * kl_divergence\n",
    "\n",
    "    # L1-norm sparsity penalty calculation\n",
    "    def l1_sparsity_penalty(self, encoded):\n",
    "        # Compute the mean of absolute values of activations\n",
    "        sparsity_loss = torch.mean(torch.abs(encoded))  # Average absolute activation across all units\n",
    "        #sparsity_loss = 0.1122  # Average absolute activation across all units\n",
    "        return self.sparsity_lambda * sparsity_loss  # Scale by the sparsity weight\n",
    "\n",
    "    # Loss function combining MSE (reconstruction error) and sparsity penalty\n",
    "    def loss_function(self, decoded, original, encoded):\n",
    "        mse_loss = F.mse_loss(decoded, original)  # Mean Squared Error for reconstruction\n",
    "        sparsity_loss = self.l1_sparsity_penalty(encoded)  # Sparsity penalty for hidden layer activations\n",
    "        return mse_loss + sparsity_loss  # Total loss is MSE + sparsity penalty\n",
    "# Instantiate the Sparse Auto-encoder with given dimensions\n",
    "def load_autoencoder_2(sae_path, device):\n",
    "    input_dim = 2048\n",
    "    hidden_dim = 8000\n",
    "    sae_2 = SparseAutoEncoder_2(input_dim, hidden_dim)\n",
    "    sae_2.load_state_dict(torch.load(sae_path, map_location=device))\n",
    "    sae_2 = sae_2.to(device)\n",
    "    # Freeze all parameters of the autoencoder\n",
    "    for param in sae_2.parameters():\n",
    "        param.requires_grad = False\n",
    "    sae_2.eval()\n",
    "    return sae_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Models\n",
    "def get_model_AlexNet(model_path, device):\n",
    "    num_features = 4096\n",
    "    num_classes = 2\n",
    "    model_path = model_path\n",
    "    device = device\n",
    "    \n",
    "    model = alexnet(weights=AlexNet_Weights.DEFAULT) # weights=AlexNet_Weights.DEFAULT) # weights=None\n",
    "    model.classifier[6] = nn.Linear(model.classifier[6].in_features, 2)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model = model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_ResNet(model_path, device):\n",
    "    num_features = 2048\n",
    "    num_classes = 2\n",
    "    model_path = model_path\n",
    "    device = device    \n",
    "    model = models.resnet50(weights=None)   #weights=ResNet50_Weights.DEFAULT)\n",
    "    model.fc = nn.Linear(model.fc.in_features, 2)  # Output for 2 classes\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model = model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def activation_correlations(b_val_patch_activations, b_val_no_patch_activations, m_val_patch_activations, m_val_no_patch_activations, act_csv_path):\n",
    "    # Step 1: Print total number of values in tensors and total number of differences\n",
    "    # Number of neurons and images\n",
    "    num_neurons = b_val_patch_activations.shape[1]\n",
    "    #print(f\"Number of val patch neurons: {num_neurons}\")\n",
    "    num_images = b_val_patch_activations.shape[0]\n",
    "    #print(f\"Number of val patch images: {b_val_patch_activations.shape[0]}\")\n",
    "\n",
    "    # Create a binary label vector pp (1 for patch, 0 for no patch)\n",
    "    pp = np.concatenate([np.ones(b_val_patch_activations.shape[0]), np.ones(m_val_patch_activations.shape[0]),\n",
    "                         np.zeros(b_val_no_patch_activations.shape[0]), np.zeros(m_val_no_patch_activations.shape[0])])\n",
    "    \n",
    "    #print(f\"Number of pp labels: {len(pp)}\")\n",
    "    \n",
    "    b_val_patch_activations = b_val_patch_activations.cpu().numpy()\n",
    "    b_val_no_patch_activations = b_val_no_patch_activations.cpu().numpy()\n",
    "    m_val_patch_activations = m_val_patch_activations.cpu().numpy()\n",
    "    m_val_no_patch_activations = m_val_no_patch_activations.cpu().numpy()\n",
    "    # array to store correlations\n",
    "    correlations = np.zeros(num_neurons)  # Shape: (2048,)\n",
    "    for i in range(num_neurons):\n",
    "        # Combine activations for neuron i from both datassets p and np\n",
    "        act_i = np.concatenate([b_val_patch_activations[:, i], m_val_patch_activations[:, i],\n",
    "                                b_val_no_patch_activations[:, i], m_val_no_patch_activations[:, i]])\n",
    "\n",
    "        # Compute correlation between pp and act_i\n",
    "        if np.std(pp) > 0 and np.std(act_i) > 0:\n",
    "            corr = np.corrcoef(pp, act_i)[0, 1]\n",
    "        else:\n",
    "            corr = 0  # Handle constant vectors\n",
    "        #corr_value = np.abs(corr)\n",
    "        correlations[i] = corr\n",
    "        \n",
    "    correlations = np.nan_to_num(correlations)  # Replace NaN values with 0\n",
    "    # Create a DataFrame with neuron indices and their correlations\n",
    "    neuron_data = pd.DataFrame({\n",
    "        \"Neuron_Index\": np.arange(num_neurons),\n",
    "        \"Correlation\": correlations\n",
    "    })\n",
    "    # Sort by correlation in descending order and vsave neurons to csv\n",
    "    neuron_data.sort_values(by=\"Correlation\", ascending=False, inplace=True)\n",
    "    # Save the DataFrame to a CSV file\n",
    "    csv_path = act_csv_path\n",
    "    neuron_data.to_csv(csv_path, index=False)\n",
    "    \n",
    "    return correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def activation_correlations_2(b_val_patch_activations, b_val_no_patch_activations, act_csv_path):\n",
    "    # Step 1: Print total number of values in tensors and total number of differences\n",
    "    # Number of neurons and images\n",
    "    num_neurons = b_val_patch_activations.shape[1]\n",
    "    #print(f\"Number of val patch neurons: {num_neurons}\")\n",
    "    num_images = b_val_patch_activations.shape[0]\n",
    "    #print(f\"Number of val patch images: {b_val_patch_activations.shape[0]}\")\n",
    "\n",
    "    # Create a binary label vector pp (1 for patch, 0 for no patch)\n",
    "    pp = np.concatenate([np.ones(b_val_patch_activations.shape[0]), np.zeros(b_val_no_patch_activations.shape[0])])\n",
    "    \n",
    "    #print(f\"Number of pp labels: {len(pp)}\")\n",
    "    b_val_patch_activations = b_val_patch_activations.cpu().numpy()\n",
    "    b_val_no_patch_activations = b_val_no_patch_activations.cpu().numpy()\n",
    "    \n",
    "    # array to store correlations\n",
    "    correlations = np.zeros(num_neurons)  # Shape: (2048,)\n",
    "    for i in range(num_neurons):\n",
    "        # Combine activations for neuron i from both datassets p and np\n",
    "        act_i = np.concatenate([b_val_patch_activations[:, i], b_val_no_patch_activations[:, i]])\n",
    "        # Compute correlation between pp and act_i\n",
    "        if np.std(pp) > 0 and np.std(act_i) > 0:\n",
    "            corr = np.corrcoef(pp, act_i)[0, 1]\n",
    "        else:\n",
    "            corr = 0  # Handle constant vectors\n",
    "        #corr_value = np.abs(corr)\n",
    "        correlations[i] = corr\n",
    "        \n",
    "    correlations = np.nan_to_num(correlations)  # Replace NaN values with 0\n",
    "    # Create a DataFrame with neuron indices and their correlations\n",
    "    neuron_data = pd.DataFrame({\n",
    "        \"Neuron_Index\": np.arange(num_neurons),\n",
    "        \"Correlation\": correlations\n",
    "    })\n",
    "    # Sort by correlation in descending order and vsave neurons to csv\n",
    "    neuron_data.sort_values(by=\"Correlation\", ascending=False, inplace=True)\n",
    "    # Save the DataFrame to a CSV file\n",
    "    csv_path = act_csv_path\n",
    "    neuron_data.to_csv(csv_path, index=False)\n",
    "    \n",
    "    return correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get activations # 2\n",
    "def get_activations_2(model, feature_extractor, dataloader):\n",
    "    labels_spu = []\n",
    "    labels_all = []\n",
    "    all_activations = []\n",
    "    spu_activations = []\n",
    "    all_class_scores = []\n",
    "    correlations = []\n",
    "    # Extract activations\n",
    "    with torch.no_grad():  # No gradient calculation for inference\n",
    "        for images, labels, benign_malignant, patches in dataloader:\n",
    "            images = images.to(device)  # Send to GPU if available\n",
    "            #images = images.unsqueeze(0)\n",
    "            labels = labels.to(device)\n",
    "            benign_malignant = benign_malignant.to(device)\n",
    "            patches = patches.to(device)\n",
    "            \n",
    "            #if val_loader_name == 'ISIC_testLoader_dataset_malignant':    \n",
    "            for label, patch in zip(benign_malignant, patches):\n",
    "                if patch.item() == 0:\n",
    "                    activations = feature_extractor(images)  # Get activations from avgpool\n",
    "                    activations = activations.view(activations.size(0), -1)  # Flatten avgpool output\n",
    "                    all_activations.append(activations.cpu())  # Collect activations and move to CPU\n",
    "                    labels_all.append(label)\n",
    "                    #class_scores = activations.gather(1, labels.view(-1, 1)).squeeze()\n",
    "                    #all_class_scores.append(class_scores.cpu())\n",
    "                elif patch.item() == 1:\n",
    "                    activations = feature_extractor(images)  # Get activations from avgpool\n",
    "                    activations = activations.view(activations.size(0), -1)  # Flatten avgpool output\n",
    "                    spu_activations.append(activations.cpu())  # Collect activations and move to CPU\n",
    "                    labels_spu.append(label)\n",
    "            \n",
    "    #print(\"all_class_scores\", all_class_scores)\n",
    "    labels_all = torch.tensor(labels_all).to(device)\n",
    "    labels_spu = torch.tensor(labels_spu).to(device)\n",
    "    all_activations = np.vstack(all_activations) # No furtehr needed\n",
    "    all_activations = torch.tensor(all_activations).to(device)\n",
    "    spu_activations = np.vstack(spu_activations) # No furtehr needed\n",
    "    spu_activations = torch.tensor(spu_activations).to(device)\n",
    "    return all_activations, labels_all, spu_activations, labels_spu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get activations\n",
    "def get_activations_AlexNet(model, dataloader, device):\n",
    "    model.eval()\n",
    "    activations = []\n",
    "    def hook_fn(module, input, output):\n",
    "        activations.append(output.detach().cpu())\n",
    "    handle = model.classifier[4].register_forward_hook(hook_fn)  # fc2 linear layer (pre-ReLU)\n",
    "    with torch.no_grad():\n",
    "        for images, _, _, _ in dataloader:\n",
    "            _ = model(images.to(device))\n",
    "    handle.remove()\n",
    "    act_tensors = torch.cat(activations, dim=0).squeeze() \n",
    "    return act_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load top neurons from CSV based on a percentage\n",
    "def load_top_neurons_from_csv(csv_path, percentage):\n",
    "    \"\"\"\n",
    "    Load top neurons based on the specified percentage from the saved CSV file.\n",
    "    \"\"\"\n",
    "    neuron_data = pd.read_csv(csv_path)\n",
    "\n",
    "    # Calculate the number of top neurons to select\n",
    "    top_count = int(len(neuron_data) * (percentage / 100))\n",
    "\n",
    "    # Select the top neurons based on their correlation difference\n",
    "    top_neurons = neuron_data.iloc[:top_count][\"Neuron_Index\"].values\n",
    "\n",
    "    # Debugging for 0% muting\n",
    "    if percentage == 0:\n",
    "        assert len(top_neurons) == 0, \"Top neurons list should be empty for 0% muting.\"\n",
    "\n",
    "    #print(f\"Loaded top {percentage}% neurons ({top_count} neurons) for muting.\")\n",
    "    return top_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations_ResNet(model, dataloader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    activations = []\n",
    "    def hook_fn(module, input, output):\n",
    "        activations.append(output.detach().cpu())\n",
    "    handle = model.avgpool.register_forward_hook(hook_fn)\n",
    "    with torch.no_grad():\n",
    "        for images, labels, _, _ in dataloader:\n",
    "            images = images.to(device)\n",
    "            _ = model(images)\n",
    "    handle.remove()\n",
    "    act_tensor = torch.cat(activations, dim=0).squeeze()  # shape: (N, 2048)\n",
    "    #act_tensor = act_tensor / (act_tensor.norm(dim=1, keepdim=True) + 1e-8)\n",
    "    # Cleanup to free memory\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return act_tensor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-group accuracy\n",
    "def calculate_group_accuracy(predictions, true_labels):\n",
    "    return accuracy_score(predictions, true_labels)\n",
    "\n",
    "def classify_with_RestNet(model, all_activations):  # changed Original\n",
    "    correct = 0 \n",
    "    total = 0\n",
    "    pruned_predictions = []\n",
    "    #with torch.enable_grad():\n",
    "    for act  in all_activations:\n",
    "        activation_tensor = act.to(device)\n",
    "        output = model.fc(activation_tensor)\n",
    "        prediction = torch.argmax(torch.nn.functional.softmax(output, dim=0)).item()\n",
    "        #prediction = torch.argmax(output).item()\n",
    "        pruned_predictions.append(prediction)\n",
    "    return pruned_predictions\n",
    "\n",
    "def classify_with_RestNet_2(model, all_activations):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    pruned_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for act  in all_activations:\n",
    "            if act.ndim == 3:\n",
    "                    activation_tensor = act.unsqueeze(0).to(device)  # [1, C, H, W]\n",
    "            else:\n",
    "                activation_tensor = act.to(device)\n",
    "            activation_tensor = act.unsqueeze(0).to(device)\n",
    "            \n",
    "            #out = model.layer4[-1].relu(activation_tensor)# apply the skipped ReLU\n",
    "            #out = model.avgpool(out)                      # global average pooling\n",
    "            #out = torch.flatten(out, 1)\n",
    "            output = model.fc(activation_tensor)                        # classification logits\n",
    "            #preds = torch.argmax(logits, dim=1)\n",
    "            #output = model.fc(activation_tensor)\n",
    "            #prediction = torch.argmax(torch.nn.functional.softmax(output, dim=0)).item()\n",
    "            prediction = torch.argmax(output, dim=1).item()\n",
    "            pruned_predictions.append(prediction)\n",
    "    return pruned_predictions\n",
    "\n",
    "def classify_with_AlexNet(model, all_activations):\n",
    "    model.eval() \n",
    "    pruned_predictions = []\n",
    "    for activation in all_activations:\n",
    "        # Convert numpy activation to tensor\n",
    "        activation_tensor = activation.to(device)\n",
    "        #activation_tensor = torch.from_numpy(activation).float().to(device)\n",
    "        relu_output = model.classifier[5](activation_tensor)  # Apply ReLU\n",
    "        output = model.classifier[6](relu_output)  # Apply fc3\n",
    "        prediction = torch.argmax(output).item()\n",
    "        #prediction = torch.argmax(torch.nn.functional.softmax(output, dim=0)).item()\n",
    "        pruned_predictions.append(prediction)\n",
    "    return pruned_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project activations into sparse space\n",
    "def project_to_sae(sae, all_activations, device):\n",
    "    #sae.to(device)\n",
    "    with torch.no_grad():\n",
    "        projected = sae.encoder(torch.from_numpy(all_activations).float().to(device))\n",
    "    return projected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# extra 1, Testing on creating correlation based on both benign and malignant labels\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_path = 'ResNet_WB_best_seed-2.pth'\n",
    "sae_path = 'ResNet_WB_SAE_seed-2.pth'\n",
    "sparse_act_csv_path = 'ResNet-on-WB_activations.csv'\n",
    "Avg_group_acc = []\n",
    "Avg_worst_group_acc = []\n",
    "Acc_LB_core = [] \n",
    "Acc_LB_spu = [] \n",
    "Acc_WB_core = [] \n",
    "Acc_WB_spu = [] \n",
    "x = [0.3, 0.6, 1.0, 1.5, 2, 5, 10, 40]\n",
    "# Get model and feature_extractor\n",
    "model = get_model_ResNet(model_path, device)\n",
    "\n",
    "LB_val_loader_p =   val_Loader_LB_patch_200\n",
    "LB_val_loader_np =  val_Loader_LB_no_patch_200\n",
    "WB_val_loader_p =   val_Loader_WB_patch_200     \n",
    "WB_val_loader_np =  val_Loader_WB_no_patch_200      \n",
    "\n",
    "test_LB_loader_p =  test_Loader_LB_patch      \n",
    "test_LB_loader_np = test_Loader_LB_no_patch \n",
    "test_WB_loader_p =  test_Loader_WB_patch\n",
    "test_WB_loader_np = test_Loader_WB_no_patch\n",
    "\n",
    "# Get activations \n",
    "LB_val_activations_no_patch  = get_activations_ResNet(model, LB_val_loader_np, device)\n",
    "LB_val_activations_patch  = get_activations_ResNet(model, LB_val_loader_p, device)\n",
    "WB_val_activations_no_patch  = get_activations_ResNet(model, WB_val_loader_np, device)\n",
    "WB_val_activations_patch  = get_activations_ResNet(model, WB_val_loader_p, device)\n",
    "\n",
    "test_LB_no_patch  = get_activations_ResNet(model, test_LB_loader_np, device)\n",
    "test_LB_patch  = get_activations_ResNet(model, test_LB_loader_p, device)\n",
    "test_WB_no_patch  = get_activations_ResNet(model, test_WB_loader_np, device)\n",
    "test_WB_patch  = get_activations_ResNet(model, test_WB_loader_p, device)\n",
    "\n",
    "print(\"classify before sparse muting.....\")\n",
    "LB_val_activations_patch_pred = classify_with_RestNet(model, LB_val_activations_patch)\n",
    "LB_val_activations_no_patch_pred = classify_with_RestNet(model, LB_val_activations_no_patch)\n",
    "test_LB_patch_pred = classify_with_RestNet(model, test_LB_patch)\n",
    "test_LB_no_patch_pred = classify_with_RestNet(model, test_LB_no_patch)\n",
    "test_WB_patch_pred = classify_with_RestNet(model, test_WB_patch)\n",
    "test_WB_no_patch_pred = classify_with_RestNet(model, test_WB_no_patch)\n",
    "# Calculate group accuracy\n",
    "LB_val_activations_patch_acc = calculate_group_accuracy(LB_val_activations_patch_pred, [0] * len(LB_val_activations_patch_pred))\n",
    "LB_val_activations_no_patch_acc = calculate_group_accuracy(LB_val_activations_no_patch_pred, [0] * len(LB_val_activations_no_patch_pred))\n",
    "test_LB_patch_acc = calculate_group_accuracy(test_LB_patch_pred, [0] * len(test_LB_patch_pred))\n",
    "test_LB_no_patch_acc = calculate_group_accuracy(test_LB_no_patch_pred, [0] * len(test_LB_no_patch_pred))\n",
    "test_WB_patch_acc = calculate_group_accuracy(test_WB_patch_pred, [1] * len(test_WB_patch_pred))\n",
    "test_WB_no_patch_acc = calculate_group_accuracy(test_WB_no_patch_pred, [1] * len(test_WB_no_patch_pred))\n",
    "print(\"Accuracy for Val. Benign_P(Spu): \", LB_val_activations_patch_acc)\n",
    "print(\"Accuracy for Va. Benign_NP(Core): \", LB_val_activations_no_patch_acc)\n",
    "\n",
    "print(\"Accuracy for LB_NP(Core): \", test_LB_no_patch_acc)\n",
    "print(\"Accuracy for LB_P(Spu): \", test_LB_patch_acc)\n",
    "print(\"Accuracy for WB_NP(Core): \", test_WB_no_patch_acc)\n",
    "print(\"Accuracy for WB_P(Spu): \", test_WB_patch_acc)\n",
    "print(\" End of classification before sparse muting......\")\n",
    "\n",
    "# The sparse space: Project activations\n",
    "sae = load_autoencoder_2(sae_path, device)\n",
    "sae.to(device)\n",
    "LB_val_activations_patch = LB_val_activations_patch.cpu().numpy()\n",
    "LB_val_activations_no_patch = LB_val_activations_no_patch.cpu().numpy()\n",
    "WB_val_activations_patch = WB_val_activations_patch.cpu().numpy()\n",
    "WB_val_activations_no_patch =  WB_val_activations_no_patch.cpu().numpy()\n",
    "test_LB_patch = test_LB_patch.cpu().numpy()\n",
    "test_LB_no_patch = test_LB_no_patch.cpu().numpy()\n",
    "test_WB_patch = test_WB_patch.cpu().numpy()\n",
    "test_WB_no_patch = test_WB_no_patch.cpu().numpy()\n",
    "\n",
    "LB_projected_val_patch = project_to_sae(sae, LB_val_activations_patch, device)\n",
    "LB_projected_val_no_patch = project_to_sae(sae, LB_val_activations_no_patch, device)\n",
    "WB_projected_val_patch = project_to_sae(sae, WB_val_activations_patch, device)\n",
    "WB_projected_val_no_patch = project_to_sae(sae, WB_val_activations_no_patch, device)\n",
    "projected_LB_test_patch = project_to_sae(sae, test_LB_patch, device)\n",
    "projected_LB_test_no_patch = project_to_sae(sae, test_LB_no_patch, device)\n",
    "projected_WB_test_patch = project_to_sae(sae, test_WB_patch, device)\n",
    "projected_WB_test_no_patch = project_to_sae(sae, test_WB_no_patch, device)\n",
    "\n",
    "# corr. \n",
    "correlations = activation_correlations(LB_projected_val_patch, LB_projected_val_no_patch, WB_projected_val_patch, WB_projected_val_no_patch, sparse_act_csv_path)\n",
    "for percentage in x:\n",
    "    # Correlaiton based Activations\n",
    "    top_neurons = load_top_neurons_from_csv(sparse_act_csv_path, percentage=percentage)\n",
    "    # Muting neurons\n",
    "    LB_projected_val_patch_muted = LB_projected_val_patch.clone().detach()\n",
    "    LB_projected_val_no_patch_muted = LB_projected_val_no_patch.clone().detach()\n",
    "    projected_WB_test_patch_muted = projected_WB_test_patch.clone().detach()\n",
    "    projected_WB_test_no_patch_muted = projected_WB_test_no_patch.clone().detach()\n",
    "    projected_LB_test_patch_muted = projected_LB_test_patch.clone().detach()\n",
    "    projected_LB_test_no_patch_muted = projected_LB_test_no_patch.clone().detach()\n",
    "    LB_projected_val_patch_muted[:, top_neurons] = 0\n",
    "    LB_projected_val_no_patch_muted[:, top_neurons] = 0\n",
    "    projected_WB_test_patch_muted[:, top_neurons] = 0\n",
    "    projected_WB_test_no_patch_muted[:, top_neurons] = 0\n",
    "    projected_LB_test_patch_muted[:, top_neurons] = 0\n",
    "    projected_LB_test_no_patch_muted[:, top_neurons] = 0\n",
    "    # Decode\n",
    "    LB_decoded_val_patch = sae.decoder(LB_projected_val_patch_muted).to(device)\n",
    "    LB_decoded_val_no_patch = sae.decoder(LB_projected_val_no_patch_muted).to(device)\n",
    "    decoded_WB_test_patch = sae.decoder(projected_WB_test_patch_muted).to(device)\n",
    "    decoded_WB_test_no_patch = sae.decoder(projected_WB_test_no_patch_muted).to(device)\n",
    "    decoded_LB_test_patch = sae.decoder(projected_LB_test_patch_muted).to(device)\n",
    "    decoded_LB_test_no_patch = sae.decoder(projected_LB_test_no_patch_muted).to(device)\n",
    "    # Classify the same amount for your \n",
    "    LB_predictions_val_patch_after = classify_with_RestNet(model, LB_decoded_val_patch)\n",
    "    LB_predictions_val_no_patch_after = classify_with_RestNet(model, LB_decoded_val_no_patch)\n",
    "    predictions_test_WB_patch_after = classify_with_RestNet(model, decoded_WB_test_patch)\n",
    "    predictions_test_WB_no_patch_after = classify_with_RestNet(model, decoded_WB_test_no_patch)\n",
    "    predictions_test_LB_patch_after = classify_with_RestNet(model, decoded_LB_test_patch)\n",
    "    predictions_test_LB_no_patch_after = classify_with_RestNet(model, decoded_LB_test_no_patch)\n",
    "    # Calculate group accuracy\n",
    "    LB_accuracy_val_patch_after = calculate_group_accuracy(LB_predictions_val_patch_after, [0] * len(LB_predictions_val_patch_after))\n",
    "    LB_accuracy_val_no_patch_after = calculate_group_accuracy(LB_predictions_val_no_patch_after, [0] * len(LB_predictions_val_no_patch_after))\n",
    "    accuracy_test_WB_patch_after = calculate_group_accuracy(predictions_test_WB_patch_after, [1] * len(predictions_test_WB_patch_after))\n",
    "    accuracy_test_WB_no_patch_after = calculate_group_accuracy(predictions_test_WB_no_patch_after, [1] * len(predictions_test_WB_no_patch_after))\n",
    "    accuracy_test_LB_patch_after = calculate_group_accuracy(predictions_test_LB_patch_after, [0] * len(predictions_test_LB_patch_after))\n",
    "    accuracy_test_LB_no_patch_after = calculate_group_accuracy(predictions_test_LB_no_patch_after, [0] * len(predictions_test_LB_no_patch_after))\n",
    "    # Worst and average group accuracies\n",
    "    AGA = (accuracy_test_WB_patch_after + accuracy_test_WB_no_patch_after + accuracy_test_LB_patch_after + accuracy_test_LB_no_patch_after) / 4\n",
    "    AWGA = min(accuracy_test_WB_patch_after, accuracy_test_WB_no_patch_after, accuracy_test_LB_patch_after, accuracy_test_LB_no_patch_after)\n",
    "    Avg_group_acc.append(AGA)\n",
    "    Avg_worst_group_acc.append(AWGA)\n",
    "    Acc_LB_core.append(accuracy_test_LB_no_patch_after)\n",
    "    Acc_LB_spu.append(accuracy_test_LB_patch_after)\n",
    "    Acc_WB_core.append(accuracy_test_WB_no_patch_after)\n",
    "    Acc_WB_spu.append(accuracy_test_WB_patch_after)\n",
    "    # Rounded values\n",
    "    Avg_group_acc_rv = [round(x * 100, 2) for x in Avg_group_acc]\n",
    "    Avg_worst_group_acc_rv = [round(x * 100, 2) for x in Avg_worst_group_acc]\n",
    "    Acc_LB_core_rv = [round(x * 100, 2) for x in Acc_LB_core]\n",
    "    Acc_LB_spu_rv = [round(x * 100, 2) for x in Acc_LB_spu]\n",
    "    Acc_WB_core_rv = [round(x * 100, 2) for x in Acc_WB_core]\n",
    "    Acc_WB_spu_rv = [round(x * 100, 2) for x in Acc_WB_spu]\n",
    "    print(\"Muting percentage x = \", percentage)\n",
    "print(\"*\" * 50)\n",
    "print(\"We apply muting percentage: x = \", x)\n",
    "print(f\"Avg_group_acc: {Avg_group_acc_rv}\")\n",
    "print(f\"Avg_worst_group_acc: {Avg_worst_group_acc_rv}\")  \n",
    "print(f\"Acc_benign_core: {Acc_LB_core_rv}\")\n",
    "print(f\"Acc_benign_spu: {Acc_LB_spu_rv}\")\n",
    "print(f\"Acc_malignant_core: {Acc_WB_core_rv}\")\n",
    "print(f\"Acc_malignant_spu: {Acc_WB_spu_rv}\")\n",
    "\n",
    "    #print(\"Prediction and Evaluation All Groups:\")\n",
    "    #prediction_and_evaluation(model, val_all_activations_decoded, val_loader, b_val_labels_all)\n",
    "print(\"*\" * 50)\n",
    "print(\"Evaluation Complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
